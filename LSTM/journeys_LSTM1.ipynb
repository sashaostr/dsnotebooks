{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self._journeys_types = np.array([\n",
    "            [1,2,3,4,5,6,7],\n",
    "            [7,6,5,4,3,2,1],\n",
    "            [4,5,2,3,1,7,6],\n",
    "            [1,1,3,4,4,5,7],\n",
    "            [1,1,1,4,4,5,5]\n",
    "    #         [5,5,5,2,2,2,3,3,3]\n",
    "        ])\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = max(map(len, self._journeys_types))\n",
    "        self._vocabulary_size = len(np.unique(self._journeys_types.flatten()))\n",
    "    \n",
    "    \n",
    "    def val2id(self, val):\n",
    "        return val-1\n",
    "    \n",
    "    \n",
    "    def id2val(self, id):\n",
    "        return id+1\n",
    "    \n",
    "    \n",
    "    def batch2journeys(self, batch):\n",
    "        r = []\n",
    "        for i in zip(*batch):\n",
    "            ids = np.argmax(i, axis=1)                \n",
    "            r.append(map(self.id2val, ids))\n",
    "        return r\n",
    "       \n",
    "        \n",
    "    def next_batch(self):\n",
    "        return [self._journeys_types[np.random.randint(0, len(self._journeys_types))] for i in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "    def encode_batches(self, batch):        \n",
    "        b_transposed = np.transpose(batch)\n",
    "        \n",
    "        res = np.zeros((self._num_unrollings, self._batch_size, self._vocabulary_size), dtype=np.float32)\n",
    "        for (unrolling, batch), value in np.ndenumerate(b_transposed):\n",
    "            res[unrolling, batch, self.val2id(value)] = 1.0   \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def encode_1h(self, id):\n",
    "        z = np.zeros(self._vocabulary_size, dtype=np.float32)        \n",
    "        z[id] = 1.0        \n",
    "#         return z.reshape((self._batch_size, self._vocabulary_size))        \n",
    "        return z.reshape((1, self._vocabulary_size))        \n",
    "        \n",
    "\n",
    "    def weighted_pick(self, weights):\n",
    "        assert len(weights) == self._vocabulary_size\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocabulary_size = 7 #total possible journey states\n",
    "n_hidden = 14 # hidden layer num of features\n",
    "n_unrollings = 7-1 #10 #max journeys length => RNN unrolled length\n",
    "batch_size = 1\n",
    "\n",
    "data_gen = BatchGenerator(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "\n",
    "    # Input data.\n",
    "    \n",
    "#     jrn_length_input = tf.placeholder(tf.int32, shape=None)\n",
    "#     jrn_length = tf.constant((jrn_length_input)\n",
    "#     jrn_length.\n",
    "#     train_data = list()\n",
    "#     for _ in range(jrn_length):\n",
    "#         train_data.append(\n",
    "#             tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "#     train_inputs = train_data[:jrn_length-1]\n",
    "#     train_labels = train_data[1:]  # labels are inputs shifted by one time step\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(n_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:n_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as rnn_scope:\n",
    "    \n",
    "        # RNN struct\n",
    "        cell = rnn_cell.LSTMCell(num_units=n_hidden, input_size=vocabulary_size)     \n",
    "        outputs, states = rnn.rnn(cell, train_inputs, dtype=tf.float32) \n",
    "        output = tf.concat(0, outputs)#tf.reshape(tf.concat(1, outputs), (-1, n_hidden))\n",
    "\n",
    "        # Classifier.\n",
    "        W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "        b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "        logits = tf.matmul(output, W_hy) + b_hy\n",
    "\n",
    "        # Loss func\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        \n",
    "#         loss_summary = tf.scalar_summary(\"loss\", loss)\n",
    "#         learning_rate_summary = tf.scalar_summary(\"learning_rate\", learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictor(inputs_number):\n",
    "    with graph.as_default():  \n",
    "        \n",
    "#         rnn_scope.reuse_variables()\n",
    "        with tf.variable_scope(\"rnn\",reuse=True) as rnn_scope:\n",
    "        \n",
    "            sample_inputs = list()\n",
    "            for _ in range(inputs_number):\n",
    "                sample_inputs.append(\n",
    "                    tf.placeholder(tf.float32, shape=[None, vocabulary_size]))\n",
    "\n",
    "            sample_outputs, sample_states = rnn.rnn(cell, sample_inputs ,dtype=tf.float32) \n",
    "\n",
    "\n",
    "            sample_output = tf.concat(0, sample_outputs)\n",
    "\n",
    "            sample_logits = tf.matmul(sample_output, W_hy) + b_hy\n",
    "            sample_prediction = tf.nn.softmax(sample_logits)\n",
    "            \n",
    "            return sample_inputs, sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.53\n",
      "[7, 4, 3, 3, 3, 7]\n",
      "Average loss at step 100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.40\n",
      "[6, 5, 4, 3, 2, 1]\n",
      "Average loss at step 200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "[1, 1, 4, 5, 5, 5]\n",
      "Average loss at step 300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "[5, 2, 3, 1, 7, 6]\n",
      "Average loss at step 400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.00\n",
      "[5, 2, 3, 1, 7, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f580a4f0ad0>> ignored\n",
      "Exception AssertionError: AssertionError() in <generator object get_controller at 0x7f580a4dd1e0> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-eeae24f9c0c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "\n",
    "# merged_summaries = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', sess.graph_def)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "# saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "\n",
    "\n",
    "print('Initialized')\n",
    "mean_loss = 0\n",
    "for step in range(num_epochs):\n",
    "\n",
    "    batches = data_gen.next_batch()\n",
    "    batches_encoded = data_gen.encode_batches(batches)\n",
    "    feed_dict = dict()\n",
    "    # fill data into input placeholders\n",
    "    for i in range(n_unrollings+1):\n",
    "        feed_dict[train_data[i]] = batches_encoded[i]\n",
    "\n",
    "\n",
    "    _, l, predictions, lr = sess.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    \n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        \n",
    "        mean_loss = 0\n",
    "        \n",
    "        \n",
    "        labels = np.concatenate(list(batches_encoded)[1:])\n",
    "        perplexity = np.exp(logprob(predictions, labels))\n",
    "        \n",
    "        perplexity_summary = tf.scalar_summary(\"perplexity\", perplexity)        \n",
    "#         mean_loss_summary = tf.scalar_summary(\"mean_loss\", mean_loss)\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            print('Minibatch perplexity: %.2f' % float(perplexity))\n",
    "#             writer.add_summary(summary_str, step)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        pr = train_prediction.eval(feed_dict)\n",
    "        print map(lambda w: data_gen.id2val(data_gen.weighted_pick(w)), pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(list(batches_encoded)[1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7)"
      ]
     },
     "execution_count": 31,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 5, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "ini_steps = [7]\n",
    "jrn_ids = map(data_gen.val2id, ini_steps)\n",
    "jrn_1h = map(data_gen.encode_1h, jrn_ids)\n",
    "\n",
    "\n",
    "# for _ in range(7-len(jrn_ids)):\n",
    "for _ in range(5-len(jrn_ids)):\n",
    "    sample_inputs, model = get_predictor(len(jrn_ids))\n",
    "\n",
    "    dick ={}\n",
    "    for i, s in zip(sample_inputs, jrn_1h):                        \n",
    "        dick[i]=s\n",
    "\n",
    "    prediction = model.eval(dick)\n",
    "\n",
    "    pred_step_id = data_gen.weighted_pick(prediction[-1].ravel())                                          \n",
    "    jrn_ids.append(pred_step_id)                 \n",
    "\n",
    "    pred_step_1h = data_gen.encode_1h(pred_step_id)                    \n",
    "    jrn_1h.append(pred_step_1h)                    \n",
    "\n",
    "\n",
    "\n",
    "print map(lambda w: data_gen.id2val(w), jrn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e85ea2bbb5c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "id=5\n",
    "\n",
    "z = np.zeros(7, dtype=np.float32)        \n",
    "z[id] = 1.0        \n",
    "z.reshape((2, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}