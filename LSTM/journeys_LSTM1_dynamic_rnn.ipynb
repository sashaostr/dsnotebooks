{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self._journeys_types = np.array([\n",
    "            [1,2,3,4,5,6,7],\n",
    "            [7,6,5,4,3,2,1],\n",
    "            [4,5,2,3,1,7,6],\n",
    "            [1,1,3,4,4,5,7],\n",
    "            [1,1,1,4,4,5,5]\n",
    "    #         [5,5,5,2,2,2,3,3,3]\n",
    "        ])\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = max(map(len, self._journeys_types))\n",
    "        self._vocabulary_size = len(np.unique(self._journeys_types.flatten()))\n",
    "    \n",
    "    \n",
    "    def val2id(self, val):\n",
    "        return val-1\n",
    "    \n",
    "    \n",
    "    def id2val(self, id):\n",
    "        return id+1\n",
    "    \n",
    "    \n",
    "    def batch2journeys(self, batch):\n",
    "        r = []\n",
    "        for i in zip(*batch):\n",
    "            ids = np.argmax(i, axis=1)                \n",
    "            r.append(map(self.id2val, ids))\n",
    "        return r\n",
    "       \n",
    "        \n",
    "    def next_batch(self):\n",
    "        return [self._journeys_types[np.random.randint(0, len(self._journeys_types))] for i in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "    def encode_batches(self, batch):        \n",
    "        b_transposed = np.transpose(batch)\n",
    "        \n",
    "        res = np.zeros((self._num_unrollings, self._batch_size, self._vocabulary_size), dtype=np.float32)\n",
    "        for (unrolling, batch), value in np.ndenumerate(b_transposed):\n",
    "            res[unrolling, batch, self.val2id(value)] = 1.0   \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def encode_1h(self, id):\n",
    "        z = np.zeros(self._vocabulary_size, dtype=np.float32)        \n",
    "        z[id] = 1.0        \n",
    "        return z.reshape((self._batch_size, self._vocabulary_size))        \n",
    "        \n",
    "\n",
    "    def weighted_pick(self, weights):\n",
    "        assert len(weights) == self._vocabulary_size\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocabulary_size = 7 #total possible journey states\n",
    "n_hidden = 14 # hidden layer num of features\n",
    "n_unrollings = 7-1 #10 #max journeys length => RNN unrolled length\n",
    "batch_size = 10\n",
    "\n",
    "data_gen = BatchGenerator(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (?, 7) and (?, 10, 7) must have the same rank",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e22fef425d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Loss func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         loss = tf.reduce_mean(\n\u001b[1;32m---> 46\u001b[1;33m                 tf.nn.softmax_cross_entropy_with_logits(logits, train_labels))\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;31m#                 tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[1;34m(logits, labels, name)\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[1;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[1;32m--> 185\u001b[1;33m       logits, labels, name=name)\n\u001b[0m\u001b[0;32m    186\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[1;34m(features, labels, name)\u001b[0m\n\u001b[0;32m    636\u001b[0m   \"\"\"\n\u001b[0;32m    637\u001b[0m   return _op_def_lib.apply_op(\"SoftmaxCrossEntropyWithLogits\",\n\u001b[1;32m--> 638\u001b[1;33m                               features=features, labels=labels, name=name)\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    653\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    654\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Restructure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2041\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1526\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1527\u001b[0m                          % op.type)\n\u001b[1;32m-> 1528\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_SoftmaxCrossEntropyWithLogitsShape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[0mlogits_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m   \u001b[0mlabels_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mmerge_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    539\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m       \u001b[0mnew_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36massert_same_rank\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    582\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         raise ValueError(\n\u001b[1;32m--> 584\u001b[1;33m             \"Shapes %s and %s must have the same rank\" % (self, other))\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0massert_has_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (?, 7) and (?, 10, 7) must have the same rank"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "\n",
    "    # Input data.    \n",
    "#     train_data = list()\n",
    "#     for _ in range(n_unrollings + 1):\n",
    "#         train_data.append(\n",
    "#             tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "#     train_inputs = train_data[:n_unrollings]\n",
    "#     train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "#     train_data = list()\n",
    "    jrn_length = tf.placeholder(tf.int32)\n",
    "#     for _ in tf.range(jrn_length):    \n",
    "#         train_data.append(\n",
    "#             tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))    \n",
    "#     train_inputs = train_data[:len(train_data)-1]\n",
    "#     train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    train_inputs = tf.placeholder(tf.float32, shape=(None, batch_size, vocabulary_size))\n",
    "    train_labels = tf.placeholder(tf.float32, shape=(None, batch_size, vocabulary_size))\n",
    "#     train_data = tf.unpack(input_batch)       \n",
    "#     jrn_length = len(train_data)       \n",
    "\n",
    "#     train_inputs = train_data[:jrn_length-1]\n",
    "#     train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as rnn_scope:\n",
    "    \n",
    "        # RNN struct\n",
    "        cell = rnn_cell.LSTMCell(num_units=n_hidden, input_size=n_hidden)  \n",
    "        output, state = rnn.dynamic_rnn(cell, train_inputs, jrn_length, time_major=True, dtype=tf.float32)\n",
    "#         outputs, states = rnn.rnn(cell, train_inputs, dtype=tf.float32) \n",
    "#         output = tf.concat(0, outputs)\n",
    "\n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "        b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "        logits = tf.matmul(output, W_hy) + b_hy\n",
    "\n",
    "        # Loss func\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, train_labels))\n",
    "#                 tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "                \n",
    "\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     with tf.variable_scope('inference'):\n",
    "        rnn_scope.reuse_variables()\n",
    "\n",
    "        sample_inputs = [tf.placeholder(tf.float32, shape=[None, vocabulary_size])]  \n",
    "        sample_outputs, sample_states = rnn.rnn(cell, sample_inputs ,dtype=tf.float32) \n",
    "\n",
    "        \n",
    "        sample_output = tf.concat(0, sample_outputs)\n",
    "\n",
    "        sample_logits = tf.matmul(sample_output, W_hy) + b_hy\n",
    "        sample_prediction = tf.nn.softmax(sample_logits)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10), Dimension(7)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.input_size, cell.output_size, cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/RNN/TensorArrayPack:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn/add:0' shape=(?, 7) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_2:0' shape=(?, 10, 7) dtype=float32>,\n",
       " TensorShape([Dimension(None), Dimension(10), Dimension(7)]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, train_labels, tf.concat(0, train_labels).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "# sess = tf.InteractiveSession(graph=graph)\n",
    "    tf.initialize_all_variables().run()\n",
    "    # saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_epochs):\n",
    "\n",
    "        batches = data_gen.next_batch()\n",
    "        batches_encoded = data_gen.encode_batches(batches)\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        feed_dict[jrn] = len(batches_encoded) - 1\n",
    "        feed_dict[input_batch] = batches_encoded\n",
    "        \n",
    "        # fill data into input placeholders\n",
    "#         for i in range(n_unrollings+1):\n",
    "#             feed_dict[train_data[i]] = batches_encoded[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        _, l, predictions, lr,jjj= sess.run([optimizer, loss, train_prediction, learning_rate,jj], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "\n",
    "        if step % summary_frequency == 0:    \n",
    "            print jjj\n",
    "            \n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches_encoded)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "\n",
    "\n",
    "            pr = train_prediction.eval(feed_dict)\n",
    "            print map(lambda w: data_gen.id2val(data_gen.weighted_pick(w)), pr)\n",
    "\n",
    "\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(1):\n",
    "                feed = data_gen.encode_1h(data_gen.val2id(4))\n",
    "                jrn = [feed.ravel()]\n",
    "                \n",
    "                for _ in range(6):\n",
    "                    prediction = sample_prediction.eval({sample_inputs[0]: feed})\n",
    "                    jrn.append(prediction.ravel())                                            \n",
    "                print map(lambda w: data_gen.id2val(data_gen.weighted_pick(w)), jrn)\n",
    "            print('=' * 80)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = rnn_cell.BasicLSTMCell(num_units=n_hidden) \n",
    "X = n_unrollings * [tf.placeholder(tf.float32,shape=(batch_size, n_hidden))] \n",
    "\n",
    "outputs, _ = rnn.rnn(cell, X, dtype=tf.float32) \n",
    "output = tf.concat(0, outputs)#tf.reshape(tf.concat(1, outputs), (-1, n_hidden))\n",
    "\n",
    "# Input data.\n",
    "train_data = list()\n",
    "for _ in range(n_unrollings + 1):\n",
    "    train_data.append(\n",
    "        tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "train_inputs = train_data[:n_unrollings]\n",
    "train_labels = train_data[1:]  # labels are inputs shifted by one time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'X: ', len(X), X[0].get_shape()\n",
    "print 'output: ', len(outputs),' * ',outputs[0].get_shape(),' => ', output.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train_inputs), train_inputs[0].get_shape(), ' => ',tf.concat(0, train_inputs).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifier.\n",
    "W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "logits = tf.matmul(output, W_hy) + b_hy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'logits: ',output.get_shape() ,\"*\",W_hy.get_shape(),\"+\",b_hy.get_shape(),'=>'\n",
    "print '    ',logits.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "\n",
    "# Optimizer.\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
