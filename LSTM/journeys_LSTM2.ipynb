{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def slidingWindow(sequence,winSize,step=1):\n",
    "    \"\"\"Returns a generator that will iterate through\n",
    "    the defined chunks of input sequence.  Input sequence\n",
    "    must be iterable.\"\"\"\n",
    " \n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # Pre-compute number of chunks to emit\n",
    "    numOfChunks = ((len(sequence)-winSize)/step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        yield sequence[i:i+winSize]\n",
    "        \n",
    "        \n",
    "def adjust_sequence_to_length(sequence, length, padder):        \n",
    "    if len(sequence) > length:\n",
    "        slices_num = float(len(sequence))/length\n",
    "        step = max(int((slices_num%1)*length),2)\n",
    "        chunks = list(slidingWindow(sequence, winSize=length, step=step))\n",
    "        return chunks\n",
    "    if len(sequence) == length:\n",
    "        return [sequence]\n",
    "    if len(sequence) < length:\n",
    "        gap = length - len(sequence)        \n",
    "        seq_pred = padder(sequence, gap)            \n",
    "        return [seq_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        self._journeys_types = np.array([\n",
    "            [1,2,3,4,5,6,7],\n",
    "            [1,2,3,4,5],\n",
    "            [7,6,5,4,3,2,1],\n",
    "            [7,6,5,4,5],\n",
    "            [4,5,2,3,1,7,6],\n",
    "            [1,1,3,4,4,5,7],\n",
    "            [1,1,3,4,4],\n",
    "            [1,1,1,4,4,5,5],\n",
    "            [5,5,5,2,2,2,3,3,3]\n",
    "        ])\n",
    "        self._batch_size = batch_size        \n",
    "        self._vocabulary_size = len(np.unique([item for sublist in self._journeys_types for item in sublist]))\n",
    "    \n",
    "    \n",
    "    def val2id(self, val):\n",
    "        return val-1\n",
    "    \n",
    "    \n",
    "    def id2val(self, id):\n",
    "        return id+1\n",
    "    \n",
    "    \n",
    "    def batch2journeys(self, batch):\n",
    "        r = []\n",
    "        for i in zip(*batch):\n",
    "            ids = np.argmax(i, axis=1)                \n",
    "            r.append(map(self.id2val, ids))\n",
    "        return r\n",
    "       \n",
    "        \n",
    "    def next_batch(self, unrollings_adjuster=None, num_unrollings=None):\n",
    "        batch = []\n",
    "        while len(batch)<self._batch_size:\n",
    "            b = self._journeys_types[np.random.randint(0, len(self._journeys_types))]  \n",
    "            if unrollings_adjuster and num_unrollings:\n",
    "                pb = unrollings_adjuster(b, num_unrollings)                 \n",
    "                batch.extend(pb)      \n",
    "            else:\n",
    "                batch.append(b)\n",
    "        return map(lambda i: batch[i], np.random.choice(len(batch), self._batch_size, replace=False))\n",
    "               \n",
    "        \n",
    "    def encode_batches(self, batch, num_unrollings):        \n",
    "        b_transposed = np.transpose(batch)\n",
    "        \n",
    "        res = np.zeros((num_unrollings, self._batch_size, self._vocabulary_size), dtype=np.float32)\n",
    "        for (unrolling, batch), value in np.ndenumerate(b_transposed):\n",
    "            res[unrolling, batch, self.val2id(value)] = 1.0   \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def encode_1h(self, id):\n",
    "        z = np.zeros(self._vocabulary_size, dtype=np.float32)        \n",
    "        z[id] = 1.0        \n",
    "        return z.reshape((1, self._vocabulary_size))        \n",
    "        \n",
    "\n",
    "    def weighted_pick(self, weights):\n",
    "        assert len(weights) == self._vocabulary_size\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_hidden = 14 # hidden layer num of features\n",
    "n_unrollings = 6 #10 #max journeys length => RNN unrolled length\n",
    "batch_size = 3\n",
    "\n",
    "data_gen = BatchGenerator(batch_size)#, n_unrollings+1)\n",
    "\n",
    "vocabulary_size = data_gen._vocabulary_size #total possible journey states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(n_unrollings+1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:n_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    train_labels_concat = tf.concat(0, train_labels)\n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as rnn_scope:\n",
    "    \n",
    "        # RNN struct\n",
    "        cell = rnn_cell.LSTMCell(num_units=n_hidden, input_size=vocabulary_size)     \n",
    "        outputs, states = rnn.rnn(cell, train_inputs, dtype=tf.float32) \n",
    "        output = tf.concat(0, outputs)#tf.reshape(tf.concat(1, outputs), (-1, n_hidden))\n",
    "\n",
    "        # Classifier.\n",
    "        W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "        b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "        logits = tf.matmul(output, W_hy) + b_hy\n",
    "\n",
    "        # Loss func\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, train_labels_concat))\n",
    "\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name='global_step')\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        \n",
    "        #perplexity\n",
    "#         predictions[predictions < 1e-10] = 1e-10\n",
    "#         return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]        \n",
    "        perplexity = tf.reduce_sum(tf.mul(train_labels_concat, -tf.log(train_prediction)))/train_labels_concat.get_shape()[0].value\n",
    "        \n",
    "        loss_summary = tf.scalar_summary(\"loss\", loss)\n",
    "        learning_rate_summary = tf.scalar_summary(\"learning_rate\", learning_rate)\n",
    "        perplexity_summary = tf.scalar_summary(\"perplexity_summary\", perplexity)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = {}\n",
    "def get_predictor(inputs_number):\n",
    "    \n",
    "    if inputs_number in predictors:       \n",
    "        return predictors[inputs_number]\n",
    "    else:\n",
    "    \n",
    "        with graph.as_default():  \n",
    "\n",
    "    #         rnn_scope.reuse_variables()\n",
    "            with tf.variable_scope(\"rnn\",reuse=True) as rnn_scope:\n",
    "\n",
    "                sample_inputs = list()\n",
    "                for _ in range(inputs_number):\n",
    "                    sample_inputs.append(\n",
    "                        tf.placeholder(tf.float32, shape=[None, vocabulary_size]))\n",
    "\n",
    "                sample_outputs, sample_states = rnn.rnn(cell, sample_inputs ,dtype=tf.float32) \n",
    "\n",
    "\n",
    "                sample_output = tf.concat(0, sample_outputs)\n",
    "\n",
    "                sample_logits = tf.matmul(sample_output, W_hy) + b_hy\n",
    "                sample_prediction = tf.nn.softmax(sample_logits)\n",
    "\n",
    "                predictors[inputs_number] = (sample_inputs, sample_prediction)\n",
    "                return sample_inputs, sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(ini_steps, num_steps_forward):\n",
    "    \n",
    "    jrn_ids = map(data_gen.val2id, ini_steps)\n",
    "    jrn_1h = map(data_gen.encode_1h, jrn_ids)\n",
    "    \n",
    "    total_steps = num_steps_forward + len(jrn_ids)\n",
    "    for _ in range(total_steps-len(jrn_ids)):\n",
    "        sample_inputs, model = get_predictor(len(jrn_ids))\n",
    "\n",
    "        dick ={}\n",
    "        for i, s in zip(sample_inputs, jrn_1h):                        \n",
    "            dick[i]=s\n",
    "\n",
    "        prediction = model.eval(dick)\n",
    "\n",
    "        pred_step_id = data_gen.weighted_pick(prediction[-1].ravel())                                          \n",
    "        jrn_ids.append(pred_step_id)                 \n",
    "\n",
    "        pred_step_1h = data_gen.encode_1h(pred_step_id)                    \n",
    "        jrn_1h.append(pred_step_1h) \n",
    "        \n",
    "    return map(lambda w: data_gen.id2val(w), jrn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.41\n",
      "[6, 6, 4, 1, 6, 5, 3, 6, 6, 6, 6, 6, 6, 3, 1, 1, 7, 1]\n",
      "Average loss at step 100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.81\n",
      "[6, 6, 2, 5, 5, 4, 4, 4, 4, 3, 5, 4, 4, 6, 5, 5, 5, 4]\n",
      "Average loss at step 200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.48\n",
      "[5, 1, 5, 3, 3, 2, 2, 4, 5, 3, 4, 2, 3, 5, 2, 3, 5, 3]\n",
      "Average loss at step 300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.28\n",
      "[1, 1, 2, 3, 1, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 7, 7]\n",
      "Average loss at step 400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[5, 6, 1, 6, 5, 3, 3, 4, 4, 1, 5, 4, 7, 2, 5, 6, 1, 5]\n",
      "Average loss at step 500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.26\n",
      "[6, 1, 1, 5, 3, 3, 4, 4, 4, 1, 4, 5, 2, 5, 6, 1, 5, 7]\n",
      "Average loss at step 600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.24\n",
      "[2, 2, 6, 3, 3, 5, 4, 4, 4, 4, 4, 3, 5, 5, 2, 7, 7, 1]\n",
      "Average loss at step 700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[1, 1, 6, 1, 1, 5, 4, 4, 4, 4, 4, 3, 5, 5, 4, 7, 5, 5]\n",
      "Average loss at step 800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.22\n",
      "[1, 6, 1, 1, 5, 3, 4, 4, 4, 4, 3, 5, 5, 2, 6, 7, 1, 7]\n",
      "Average loss at step 900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[1, 5, 1, 3, 2, 3, 4, 3, 4, 5, 1, 4, 6, 7, 5, 7, 6, 5]\n",
      "Average loss at step 1000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.38\n",
      "[2, 6, 2, 3, 5, 3, 4, 4, 4, 4, 5, 4, 5, 2, 5, 7, 1, 5]\n",
      "Average loss at step 1100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[5, 1, 5, 2, 3, 5, 2, 4, 2, 3, 5, 2, 3, 6, 2, 3, 7, 3]\n",
      "Average loss at step 1200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[1, 5, 1, 3, 5, 3, 4, 2, 4, 5, 2, 5, 6, 2, 6, 7, 3, 7]\n",
      "Average loss at step 1300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.09\n",
      "[2, 1, 5, 3, 3, 2, 4, 4, 3, 5, 4, 1, 6, 5, 7, 7, 7, 6]\n",
      "Average loss at step 1400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.20\n",
      "[2, 2, 1, 2, 5, 3, 2, 2, 4, 3, 2, 4, 3, 2, 5, 3, 3, 7]\n",
      "Average loss at step 1500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.07\n",
      "[6, 6, 5, 5, 5, 2, 4, 4, 3, 5, 3, 1, 2, 2, 7, 1, 1, 6]\n",
      "Average loss at step 1600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.05\n",
      "[6, 6, 6, 5, 5, 5, 4, 4, 4, 3, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Average loss at step 1700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[6, 2, 1, 5, 3, 3, 4, 4, 4, 3, 4, 4, 5, 5, 5, 5, 7, 7]\n",
      "Average loss at step 1800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 4, 5, 6, 5, 6, 7, 7, 7]\n",
      "Average loss at step 1900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.27\n",
      "[1, 1, 1, 3, 3, 3, 4, 4, 4, 5, 4, 4, 6, 5, 5, 7, 5, 7]\n",
      "Average loss at step 2000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[1, 5, 2, 3, 2, 1, 4, 3, 4, 5, 1, 4, 6, 7, 5, 7, 6, 5]\n",
      "Average loss at step 2100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.37\n",
      "[5, 6, 6, 2, 5, 5, 3, 4, 4, 1, 3, 3, 7, 4, 4, 6, 3, 3]\n",
      "Average loss at step 2200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.05\n",
      "[1, 1, 5, 3, 3, 2, 4, 4, 3, 4, 4, 1, 5, 5, 7, 7, 7, 6]\n",
      "Average loss at step 2300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.18\n",
      "[2, 6, 1, 1, 5, 3, 4, 4, 4, 4, 3, 5, 5, 2, 6, 7, 1, 7]\n",
      "Average loss at step 2400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.31\n",
      "[1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 7, 7]\n",
      "Average loss at step 2500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.33\n",
      "[5, 2, 1, 2, 1, 1, 3, 4, 4, 1, 4, 4, 7, 5, 5, 6, 7, 6]\n",
      "Average loss at step 2600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[6, 2, 1, 5, 3, 3, 4, 4, 4, 3, 4, 5, 2, 5, 6, 1, 7, 7]\n",
      "Average loss at step 2700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7]\n",
      "Average loss at step 2800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.19\n",
      "[6, 5, 6, 5, 2, 5, 4, 2, 4, 3, 1, 5, 2, 7, 5, 5, 6, 5]\n",
      "Average loss at step 2900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 4, 5, 6, 5, 7, 7, 7]\n",
      "Average loss at step 3000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.03\n",
      "[5, 5, 6, 2, 2, 5, 3, 3, 4, 1, 1, 5, 7, 7, 2, 6, 6, 1]\n",
      "Average loss at step 3100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[1, 6, 5, 3, 5, 5, 4, 4, 2, 4, 3, 2, 5, 2, 2, 5, 1, 3]\n",
      "Average loss at step 3200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[1, 1, 5, 3, 3, 2, 4, 4, 3, 4, 5, 1, 5, 6, 7, 7, 7, 6]\n",
      "Average loss at step 3300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.09\n",
      "[6, 1, 6, 5, 3, 5, 4, 4, 4, 3, 5, 2, 2, 6, 2, 1, 7, 1]\n",
      "Average loss at step 3400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[1, 2, 5, 3, 3, 2, 4, 4, 3, 4, 4, 1, 5, 5, 7, 5, 7, 6]\n",
      "Average loss at step 3500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.04\n",
      "[1, 1, 6, 1, 3, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5]\n",
      "Average loss at step 3600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[2, 1, 2, 5, 3, 2, 2, 4, 2, 2, 4, 3, 2, 5, 3, 3, 7, 3]\n",
      "Average loss at step 3700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[1, 5, 1, 3, 2, 3, 4, 3, 4, 5, 1, 5, 6, 7, 6, 7, 6, 7]\n",
      "Average loss at step 3800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.39\n",
      "[2, 6, 2, 3, 5, 3, 4, 4, 4, 4, 3, 4, 5, 2, 5, 7, 2, 7]\n",
      "Average loss at step 3900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.36\n",
      "[1, 2, 1, 3, 1, 1, 4, 4, 4, 5, 4, 4, 6, 5, 5, 7, 7, 7]\n",
      "Average loss at step 4000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.20\n",
      "[5, 1, 2, 2, 3, 3, 3, 4, 4, 1, 4, 5, 7, 5, 6, 6, 5, 7]\n",
      "Average loss at step 4100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.42\n",
      "[1, 1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 5, 7, 7]\n",
      "Average loss at step 4200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[5, 5, 1, 2, 2, 1, 3, 3, 4, 1, 1, 4, 7, 7, 5, 6, 6, 7]\n",
      "Average loss at step 4300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[6, 2, 1, 5, 3, 3, 4, 4, 4, 3, 5, 4, 2, 6, 5, 2, 7, 7]\n",
      "Average loss at step 4400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.10\n",
      "[2, 2, 2, 2, 3, 5, 2, 4, 2, 3, 5, 2, 3, 6, 2, 3, 7, 3]\n",
      "Average loss at step 4500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.23\n",
      "[5, 1, 1, 5, 3, 3, 2, 4, 4, 2, 5, 5, 2, 6, 6, 3, 7, 7]\n",
      "Average loss at step 4600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.08\n",
      "[2, 2, 1, 2, 3, 3, 2, 4, 4, 3, 4, 4, 3, 5, 5, 3, 7, 7]\n",
      "Average loss at step 4700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[2, 2, 5, 2, 5, 5, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3]\n",
      "Average loss at step 4800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[2, 2, 2, 5, 3, 2, 2, 4, 2, 2, 5, 3, 2, 6, 3, 3, 7, 3]\n",
      "Average loss at step 4900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[5, 6, 5, 2, 5, 5, 2, 4, 2, 3, 3, 2, 3, 2, 2, 3, 1, 3]\n",
      "Average loss at step 5000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.25\n",
      "[6, 2, 2, 5, 5, 1, 4, 2, 4, 5, 2, 4, 4, 2, 5, 3, 3, 5]\n",
      "Average loss at step 5100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[1, 2, 5, 1, 2, 5, 4, 2, 2, 4, 3, 2, 5, 3, 2, 7, 3, 3]\n",
      "Average loss at step 5200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[2, 6, 2, 3, 5, 1, 4, 4, 4, 4, 5, 4, 5, 4, 5, 7, 3, 7]\n",
      "Average loss at step 5300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[5, 2, 1, 2, 5, 3, 2, 2, 4, 3, 2, 4, 3, 2, 5, 3, 3, 7]\n",
      "Average loss at step 5400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.21\n",
      "[1, 1, 6, 3, 1, 5, 4, 4, 4, 4, 4, 3, 5, 5, 4, 5, 7, 5]\n",
      "Average loss at step 5500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.10\n",
      "[1, 5, 1, 3, 2, 3, 4, 3, 4, 5, 1, 4, 6, 7, 5, 7, 6, 7]\n",
      "Average loss at step 5600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.19\n",
      "[1, 2, 1, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7]\n",
      "Average loss at step 5700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[5, 5, 6, 2, 5, 5, 2, 2, 4, 3, 2, 3, 3, 2, 4, 3, 3, 3]\n",
      "Average loss at step 5800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.19\n",
      "[1, 6, 2, 3, 5, 3, 4, 4, 4, 5, 5, 4, 6, 2, 5, 7, 1, 5]\n",
      "Average loss at step 5900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.10\n",
      "[5, 2, 2, 2, 3, 3, 3, 4, 4, 1, 5, 4, 7, 6, 5, 6, 7, 7]\n",
      "Average loss at step 6000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.23\n",
      "[2, 2, 1, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 5, 7, 7]\n",
      "Average loss at step 6100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "[1, 1, 5, 3, 3, 2, 4, 4, 3, 4, 4, 1, 5, 5, 7, 7, 7, 6]\n",
      "Average loss at step 6200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.21\n",
      "[6, 6, 2, 5, 5, 3, 4, 4, 4, 5, 3, 4, 4, 2, 5, 5, 1, 5]\n",
      "Average loss at step 6300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[2, 1, 2, 2, 3, 5, 2, 4, 2, 3, 4, 2, 3, 5, 2, 3, 7, 3]\n",
      "Average loss at step 6400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[2, 2, 5, 3, 5, 2, 4, 2, 2, 4, 2, 3, 5, 2, 3, 5, 3, 3]\n",
      "Average loss at step 6500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[5, 2, 2, 2, 1, 1, 3, 4, 4, 1, 4, 4, 7, 5, 5, 6, 5, 7]\n",
      "Average loss at step 6600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[2, 6, 6, 3, 5, 5, 4, 4, 4, 4, 3, 5, 5, 4, 2, 7, 3, 1]\n",
      "Average loss at step 6700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.08\n",
      "[2, 5, 2, 3, 2, 5, 4, 3, 2, 5, 1, 2, 6, 7, 2, 7, 6, 3]\n",
      "Average loss at step 6800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[5, 5, 2, 5, 2, 3, 2, 2, 4, 2, 3, 4, 2, 3, 5, 3, 3, 7]\n",
      "Average loss at step 6900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[2, 2, 1, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 6, 7, 7, 7]\n",
      "Average loss at step 7000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.10\n",
      "[2, 5, 1, 3, 2, 3, 4, 3, 4, 5, 1, 5, 6, 7, 6, 7, 6, 7]\n",
      "Average loss at step 7100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[1, 1, 5, 1, 3, 2, 4, 4, 3, 4, 4, 1, 5, 5, 7, 7, 7, 6]\n",
      "Average loss at step 7200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[5, 2, 1, 2, 2, 3, 3, 2, 4, 1, 3, 4, 7, 3, 5, 6, 3, 5]\n",
      "Average loss at step 7300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.22\n",
      "[6, 6, 2, 5, 5, 1, 4, 4, 4, 5, 3, 4, 4, 4, 5, 5, 5, 7]\n",
      "Average loss at step 7400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[5, 1, 2, 2, 3, 5, 2, 4, 2, 3, 5, 2, 3, 6, 2, 3, 7, 3]\n",
      "Average loss at step 7500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[6, 1, 2, 5, 3, 3, 4, 4, 4, 5, 4, 5, 4, 5, 6, 3, 7, 7]\n",
      "Average loss at step 7600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[1, 5, 5, 3, 5, 2, 4, 2, 2, 4, 2, 3, 5, 2, 3, 5, 3, 3]\n",
      "Average loss at step 7700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.10\n",
      "[1, 5, 1, 3, 2, 1, 4, 3, 4, 5, 1, 4, 6, 7, 5, 7, 6, 7]\n",
      "Average loss at step 7800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[6, 5, 1, 5, 2, 3, 4, 3, 4, 3, 1, 5, 4, 7, 6, 3, 6, 7]\n",
      "Average loss at step 7900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[5, 2, 6, 2, 3, 5, 3, 4, 4, 1, 4, 5, 7, 5, 2, 6, 7, 1]\n",
      "Average loss at step 8000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.14\n",
      "[5, 1, 2, 2, 3, 3, 3, 4, 4, 1, 4, 4, 7, 5, 5, 6, 5, 7]\n",
      "Average loss at step 8100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[1, 5, 1, 3, 2, 3, 4, 3, 4, 4, 1, 5, 5, 7, 6, 5, 6, 7]\n",
      "Average loss at step 8200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.25\n",
      "[1, 6, 6, 3, 5, 5, 4, 4, 4, 4, 5, 5, 5, 2, 4, 5, 1, 3]\n",
      "Average loss at step 8300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[5, 1, 2, 2, 3, 1, 3, 4, 4, 1, 4, 4, 7, 5, 5, 6, 7, 7]\n",
      "Average loss at step 8400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.16\n",
      "[2, 2, 2, 5, 3, 2, 2, 4, 2, 2, 4, 3, 2, 5, 3, 3, 5, 3]\n",
      "Average loss at step 8500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[6, 6, 6, 5, 5, 5, 4, 4, 4, 5, 3, 3, 2, 4, 2, 1, 5, 1]\n",
      "Average loss at step 8600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[2, 1, 6, 3, 3, 5, 4, 4, 4, 5, 5, 5, 6, 6, 2, 7, 7, 1]\n",
      "Average loss at step 8700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[1, 2, 1, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7]\n",
      "Average loss at step 8800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.17\n",
      "[1, 6, 1, 3, 5, 3, 4, 4, 4, 4, 5, 4, 5, 4, 5, 7, 3, 7]\n",
      "Average loss at step 8900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[2, 2, 2, 5, 3, 2, 2, 4, 2, 2, 4, 3, 2, 5, 3, 3, 7, 3]\n",
      "Average loss at step 9000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[6, 6, 6, 5, 5, 5, 4, 4, 4, 5, 3, 5, 2, 2, 2, 1, 1, 1]\n",
      "Average loss at step 9100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.20\n",
      "[2, 1, 1, 1, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 5, 7, 7]\n",
      "Average loss at step 9200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.12\n",
      "[2, 2, 2, 2, 5, 3, 2, 2, 4, 3, 2, 4, 3, 2, 5, 3, 3, 7]\n",
      "Average loss at step 9300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.25\n",
      "[2, 1, 1, 1, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 7, 5, 5]\n",
      "Average loss at step 9400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[5, 2, 6, 5, 2, 5, 2, 2, 4, 2, 3, 3, 2, 3, 2, 3, 3, 1]\n",
      "Average loss at step 9500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[5, 2, 5, 2, 3, 5, 2, 4, 2, 3, 4, 2, 3, 5, 2, 3, 7, 3]\n",
      "Average loss at step 9600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.11\n",
      "[2, 2, 5, 1, 3, 2, 4, 4, 3, 4, 5, 1, 5, 6, 7, 7, 7, 6]\n",
      "Average loss at step 9700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.13\n",
      "[6, 5, 2, 5, 5, 2, 4, 2, 2, 3, 2, 3, 4, 2, 3, 3, 3, 3]\n",
      "Average loss at step 9800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.15\n",
      "[1, 1, 1, 3, 3, 3, 4, 4, 4, 5, 5, 4, 6, 6, 5, 7, 7, 7]\n",
      "Average loss at step 9900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.27\n",
      "[6, 1, 1, 5, 1, 1, 4, 4, 4, 3, 4, 4, 4, 5, 5, 3, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "summary_frequency = 100\n",
    "\n",
    "unrollings_adjuster = partial(adjust_sequence_to_length, padder=predict)\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "\n",
    "merged_summaries = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', sess.graph_def)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "# saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "\n",
    "\n",
    "print('Initialized')\n",
    "mean_loss = 0\n",
    "for step in range(num_epochs):\n",
    "\n",
    "    batches = data_gen.next_batch(unrollings_adjuster=unrollings_adjuster, num_unrollings=n_unrollings+1)\n",
    "    batches_encoded = data_gen.encode_batches(batches,n_unrollings+1)\n",
    "    feed_dict = dict()\n",
    "    # fill data into input placeholders\n",
    "    for i in range(n_unrollings+1):\n",
    "        feed_dict[train_data[i]] = batches_encoded[i]\n",
    "\n",
    "\n",
    "    _, l, prplx, predictions, lr, summaries_str = sess.run([optimizer, loss, perplexity, train_prediction, learning_rate, merged_summaries], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    \n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        \n",
    "        mean_loss = 0\n",
    "        \n",
    "        \n",
    "#         labels = np.concatenate(list(batches_encoded)[1:])\n",
    "#         perplexity = np.exp(logprob(predictions, labels))\n",
    "        \n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            print('Minibatch perplexity: %.2f' % float(prplx))\n",
    "            writer.add_summary(summaries_str, step)\n",
    "#             writer.add_summary(perplexity_summary, step)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#         pr = train_prediction.eval(feed_dict)\n",
    "        pr = predictions\n",
    "        print map(lambda w: data_gen.id2val(data_gen.weighted_pick(w)), pr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 5, 4, 5, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([7,6], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen = BatchGenerator(2,4)\n",
    "batches = data_gen.next_batch()\n",
    "batches\n",
    "# batches_encoded = data_gen.encode_batches(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ini_steps = [2,3,1]\n",
    "jrn_ids = map(data_gen.val2id, ini_steps)\n",
    "jrn_1h = map(data_gen.encode_1h, jrn_ids)\n",
    "\n",
    "\n",
    "# for _ in range(7-len(jrn_ids)):\n",
    "for _ in range(10-len(jrn_ids)):\n",
    "    sample_inputs, model = get_predictor(len(jrn_ids))\n",
    "\n",
    "    dick ={}\n",
    "    for i, s in zip(sample_inputs, jrn_1h):                        \n",
    "        dick[i]=s\n",
    "\n",
    "    prediction = model.eval(dick)\n",
    "\n",
    "    pred_step_id = data_gen.weighted_pick(prediction[-1].ravel())                                          \n",
    "    jrn_ids.append(pred_step_id)                 \n",
    "\n",
    "    pred_step_1h = data_gen.encode_1h(pred_step_id)                    \n",
    "    jrn_1h.append(pred_step_1h)                    \n",
    "\n",
    "\n",
    "\n",
    "print map(lambda w: data_gen.id2val(w), jrn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = [1,2,3]\n",
    "print np.pad(arr,pad_width=3, mode='reflect')\n",
    "print np.pad(arr,pad_width=3, mode='symmetric')\n",
    "print np.pad(arr,pad_width=3, mode='wrap')\n",
    "print np.pad(arr,pad_width=3, mode='edge')\n",
    "print np.pad(arr,pad_width=3, mode='constant',constant_values=9)\n",
    "print np.pad(arr,pad_width=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
