{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence,winSize,step=1):\n",
    "    \"\"\"Returns a generator that will iterate through\n",
    "    the defined chunks of input sequence.  Input sequence\n",
    "    must be iterable.\"\"\"\n",
    " \n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # Pre-compute number of chunks to emit\n",
    "    numOfChunks = ((len(sequence)-winSize)/step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        yield sequence[i:i+winSize]\n",
    "        \n",
    "        \n",
    "def adjust_sequence_to_length(sequence, length, meta, padder):        \n",
    "    if len(sequence) > length:\n",
    "        slices_num = float(len(sequence))/length\n",
    "        step = max(int((slices_num%1)*length),2)\n",
    "        chunks = list(slidingWindow(sequence, winSize=length, step=step))\n",
    "        return chunks\n",
    "    if len(sequence) == length:\n",
    "        return [sequence]\n",
    "    if len(sequence) < length:\n",
    "        gap = length - len(sequence)        \n",
    "        seq_pred = padder(sequence, gap, meta)            \n",
    "        return [seq_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        self._journeys_types = np.array([\n",
    "#             [1,2,3,4,5,6,7],\n",
    "#             [1,2,3,4,5],\n",
    "#             [7,6,5,4,3,2,1],\n",
    "#             [7,6,5,4,5],\n",
    "#             [4,5,2,3,1,7,6],\n",
    "#             [1,1,3,4,4,5,7],\n",
    "#             [1,1,3,4,4],\n",
    "#             [1,1,1,4,4,5,5],\n",
    "#             [5,5,5,2,2,2,3,3,3]\n",
    "                \n",
    "                \n",
    "            ([1,2,3, 7,6,5,4],[0]),\n",
    "            ([1,2,3, 4,5,6,7],[1]),\n",
    "            ([7,6,5, 4,3,2,1],[0]),\n",
    "            ([7,6,5, 1,2,3,4],[1])\n",
    "            \n",
    "#             [1,2,3,4,5],\n",
    "#             [7,6,5,4,3,2,1],\n",
    "#             [7,6,5,4,5],\n",
    "#             [4,5,2,3,1,7,6],\n",
    "#             [1,1,3,4,4,5,7],\n",
    "#             [1,1,3,4,4],\n",
    "#             [1,1,1,4,4,5,5],\n",
    "#             [5,5,5,2,2,2,3,3,3]\n",
    "        ])\n",
    "        self._batch_size = batch_size \n",
    "#         self._meta_vocabulary_size = meta_vocabulary_size\n",
    "        \n",
    "        journeys_types = map(lambda j: j[0], self._journeys_types)\n",
    "        self._journey_vocabulary_size=len(np.unique([item for sublist in journeys_types for item in sublist]))\n",
    "        \n",
    "        meta_types = map(lambda j: j[1], self._journeys_types)\n",
    "        self._meta_vocabulary_size=len(np.unique([item for sublist in meta_types for item in sublist]))\n",
    "        \n",
    "        self._vocabulary_size = self._journey_vocabulary_size + self._meta_vocabulary_size\n",
    "    \n",
    "    \n",
    "    def val2id(self, val):\n",
    "        return val-1\n",
    "    \n",
    "    \n",
    "    def id2val(self, id):\n",
    "        return id+1\n",
    "    \n",
    "    \n",
    "    def encode_meta_1h(self, id):\n",
    "        z = np.zeros(self._meta_vocabulary_size, dtype=np.float32)        \n",
    "        z[id] = 1.0        \n",
    "        return z.reshape((1, self._meta_vocabulary_size)) \n",
    "    \n",
    "#     def batch2journeys(self, batch):\n",
    "#         r = []\n",
    "#         for i in zip(*batch):\n",
    "#             ids = np.argmax(i, axis=1)                \n",
    "#             r.append(map(self.id2val, ids))\n",
    "#         return r\n",
    "       \n",
    "        \n",
    "    def next_batch(self, unrollings_adjuster=None, num_unrollings=None):\n",
    "        batch = []\n",
    "        while len(batch)<self._batch_size:\n",
    "            jrn_md = self._journeys_types[np.random.randint(0, len(self._journeys_types))]  \n",
    "            jrn = jrn_md[0]\n",
    "            meta_data = jrn_md[1]\n",
    "            if unrollings_adjuster and num_unrollings:\n",
    "                pb = unrollings_adjuster(jrn, num_unrollings, meta_data)               \n",
    "                pb_md = map(lambda j: (j,meta_data),pb)                \n",
    "                batch.extend(pb_md)      \n",
    "            else:\n",
    "                batch.append((jrn, meta_data))\n",
    "        return map(lambda i: batch[i], np.random.choice(len(batch), self._batch_size, replace=False))\n",
    "               \n",
    "        \n",
    "    def encode_batches(self, batches, num_unrollings):   \n",
    "        \n",
    "        jrns = map(lambda b:b[0], batches)\n",
    "        metas = map(lambda b:b[1], batches)\n",
    "                \n",
    "        b_transposed = np.transpose(jrns)\n",
    "        \n",
    "        res = np.zeros((num_unrollings, self._batch_size, self._vocabulary_size), dtype=np.float32)\n",
    "        for (unrolling, batch), value in np.ndenumerate(b_transposed):\n",
    "            res[unrolling, batch, self.val2id(value)] = 1.0   \n",
    "            meta_1h = self.encode_meta_1h(metas[batch])\n",
    "#             print self._vocabulary_size-self._meta_vocabulary_size, self._vocabulary_size,'=>',meta\n",
    "            res[unrolling, batch].put([self._vocabulary_size-self._meta_vocabulary_size, self._vocabulary_size-1], meta_1h)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def encode_1h(self, jrn_step_id, meta_id=None):\n",
    "        z = np.zeros(self._vocabulary_size, dtype=np.float32)        \n",
    "        z[jrn_step_id] = 1.0        \n",
    "        \n",
    "        if meta_id:\n",
    "            meta_1h = self.encode_meta_1h(meta_id)\n",
    "            z.put([self._vocabulary_size-self._meta_vocabulary_size, self._vocabulary_size-1], meta_1h)\n",
    "        \n",
    "        return z.reshape((1, self._vocabulary_size))        \n",
    "            \n",
    "\n",
    "    def weighted_pick(self, weights):\n",
    "        assert len(weights) == self._vocabulary_size - self._meta_vocabulary_size\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unrollings_adjuster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-536c602bcedc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munrollings_adjuster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munrollings_adjuster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_unrollings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unrollings_adjuster' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "jrn_voc = 7\n",
    "meta_voc = 2\n",
    "vocabulary_size = jrn_voc+meta_voc\n",
    "num_unrollings = 3\n",
    "data_gen = BatchGenerator(batch_size)\n",
    "\n",
    "batches = data_gen.next_batch(unrollings_adjuster=unrollings_adjuster, num_unrollings=3)\n",
    "batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen.encode_batches(batches, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = map(lambda b:b[0], batches)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta = map(lambda b:b[1], batches)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_transposed = np.transpose(batch)\n",
    "b_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.zeros((num_unrollings, batch_size, vocabulary_size), dtype=np.float32)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (unrolling, batch), value in np.ndenumerate(b_transposed):\n",
    "    print unrolling, batch, value, meta[batch]\n",
    "#     res[unrolling, batch, value-1] = 1.0 \n",
    "    res[unrolling, batch].put([jrn_voc,jrn_voc+meta_voc-1], meta[batch])\n",
    "#     res[unrolling, batch, self.val2id(value)] = 1.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res[0,0].put([jrn_voc,jrn_voc+meta_voc-1],[1,1])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[1,2,3,4]])\n",
    "a[1,2] = 99\n",
    "# a.put([0,[2,3]],np.array([5,6]))\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_hidden = 14 # hidden layer num of features\n",
    "n_unrollings = 6 #10 #max journeys length => RNN unrolled length\n",
    "batch_size = 5\n",
    "\n",
    "data_gen = BatchGenerator(batch_size)\n",
    "\n",
    "meta_vocabulary_size = data_gen._meta_vocabulary_size\n",
    "total_vocabulary_size = data_gen._vocabulary_size\n",
    "\n",
    "vocabulary_size = total_vocabulary_size - meta_vocabulary_size #total possible journey states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()    \n",
    "    for _ in range(n_unrollings+1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, total_vocabulary_size]))        \n",
    "        \n",
    "    train_inputs = train_data[:n_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    train_labels_concat_ = tf.concat(0, train_labels)\n",
    "    s = train_labels_concat_.get_shape()     \n",
    "    train_labels_concat = tf.slice(train_labels_concat_,  [0, 0], np.array(s.as_list())-np.array([0,meta_vocabulary_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #social Input data\n",
    "#     social_input = tf.placeholder(tf.float32, shape=[batch_size, social_vocabulary_size])\n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as rnn_scope:\n",
    "    \n",
    "        \n",
    "    \n",
    "        # RNN struct\n",
    "        cell = rnn_cell.LSTMCell(num_units=n_hidden, input_size=total_vocabulary_size)     \n",
    "        outputs, states = rnn.rnn(cell, train_inputs, dtype=tf.float32) \n",
    "        outputs_concat = tf.concat(0, outputs)#tf.reshape(tf.concat(1, outputs), (-1, n_hidden))\n",
    "\n",
    "        # Classifier.\n",
    "        W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "        b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "        \n",
    "        \n",
    "#         W_social = tf.get_variable(\"W_social\", [n_hidden, social_vocabulary_size])\n",
    "#         b_social = tf.get_variable(\"b_social\", [social_vocabulary_size])\n",
    "        \n",
    "        logits = tf.matmul(outputs_concat, W_hy) + b_hy\n",
    "\n",
    "        # Loss func\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, train_labels_concat))\n",
    "\n",
    "        \n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name='global_step')\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        \n",
    "        #perplexity\n",
    "#         predictions[predictions < 1e-10] = 1e-10\n",
    "#         return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]        \n",
    "        perplexity = tf.reduce_sum(tf.mul(train_labels_concat, -tf.log(train_prediction)))/train_labels_concat.get_shape()[0].value\n",
    "        \n",
    "        loss_summary = tf.scalar_summary(\"loss\", loss)\n",
    "        learning_rate_summary = tf.scalar_summary(\"learning_rate\", learning_rate)\n",
    "        perplexity_summary = tf.scalar_summary(\"perplexity_summary\", perplexity)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Placeholder:0' shape=(5, 9) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(5, 9) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_2:0' shape=(5, 9) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_3:0' shape=(5, 9) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_4:0' shape=(5, 9) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_5:0' shape=(5, 9) dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 14) (14, 7) + (7,) => (30, 7)\n"
     ]
    }
   ],
   "source": [
    "print outputs_concat.get_shape(), W_hy.get_shape(), '+',b_hy.get_shape(),'=>', logits.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 7)\n"
     ]
    }
   ],
   "source": [
    "print train_prediction.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnn/RNN/LSTMCell/mul_2:0' shape=(5, 14) dtype=float32>,\n",
       " <tf.Tensor 'rnn/RNN/LSTMCell_1/mul_2:0' shape=(5, 14) dtype=float32>,\n",
       " <tf.Tensor 'rnn/RNN/LSTMCell_2/mul_2:0' shape=(5, 14) dtype=float32>,\n",
       " <tf.Tensor 'rnn/RNN/LSTMCell_3/mul_2:0' shape=(5, 14) dtype=float32>,\n",
       " <tf.Tensor 'rnn/RNN/LSTMCell_4/mul_2:0' shape=(5, 14) dtype=float32>,\n",
       " <tf.Tensor 'rnn/RNN/LSTMCell_5/mul_2:0' shape=(5, 14) dtype=float32>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'concat:0' shape=(30, 9) dtype=float32>,\n",
       " <tf.Tensor 'Slice:0' shape=(30, 7) dtype=float32>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_concat_,train_labels_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = {}\n",
    "def get_predictor(inputs_number):\n",
    "    \n",
    "    if inputs_number in predictors:       \n",
    "        return predictors[inputs_number]\n",
    "    else:\n",
    "    \n",
    "        with graph.as_default():  \n",
    "\n",
    "    #         rnn_scope.reuse_variables()\n",
    "            with tf.variable_scope(\"rnn\",reuse=True) as rnn_scope:\n",
    "\n",
    "                sample_inputs = list()\n",
    "                for _ in range(inputs_number):\n",
    "                    sample_inputs.append(\n",
    "                        tf.placeholder(tf.float32, shape=[None, total_vocabulary_size]))\n",
    "\n",
    "                sample_outputs, sample_states = rnn.rnn(cell, sample_inputs ,dtype=tf.float32) \n",
    "\n",
    "\n",
    "                sample_output = tf.concat(0, sample_outputs)\n",
    "\n",
    "                sample_logits = tf.matmul(sample_output, W_hy) + b_hy\n",
    "                sample_prediction = tf.nn.softmax(sample_logits)\n",
    "\n",
    "                predictors[inputs_number] = (sample_inputs, sample_prediction)\n",
    "                return sample_inputs, sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(ini_steps, num_steps_forward, meta_id):\n",
    "    \n",
    "    jrn_ids = map(data_gen.val2id, ini_steps)\n",
    "    encode_jrn_1h = partial(data_gen.encode_1h, meta_id=meta_id)\n",
    "    jrn_1h = map(encode_jrn_1h, jrn_ids)\n",
    "    \n",
    "    total_steps = num_steps_forward + len(jrn_ids)\n",
    "    for _ in range(total_steps-len(jrn_ids)):\n",
    "        sample_inputs, model = get_predictor(len(jrn_ids))\n",
    "\n",
    "        dick ={}\n",
    "        for i, s in zip(sample_inputs, jrn_1h):                        \n",
    "            dick[i]=s\n",
    "\n",
    "        prediction = model.eval(dick)\n",
    "\n",
    "        #take only last step\n",
    "        last_pred_step = prediction[-1].ravel()\n",
    "        pred_step_id = data_gen.weighted_pick(last_pred_step)                                          \n",
    "        jrn_ids.append(pred_step_id)                 \n",
    "\n",
    "        pred_step_1h = data_gen.encode_1h(pred_step_id, meta_id=meta_id)                    \n",
    "        jrn_1h.append(pred_step_1h) \n",
    "        \n",
    "    return map(lambda w: data_gen.id2val(w), jrn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.25\n",
      "[6, 6, 6, 4, 6, 6, 6, 6, 6, 4, 6, 6, 3, 5, 6, 3, 4, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4]\n",
      "Average loss at step 100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 2, 5, 5, 5, 3, 3, 1, 1, 1, 7, 4, 2, 2, 2, 6, 5, 3, 3, 3, 5, 6, 4, 4, 4, 4, 7]\n",
      "Average loss at step 200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 4, 4, 1, 1, 4, 3, 5, 2, 2, 5, 2, 6, 3, 3, 6, 1, 7, 4, 4, 7]\n",
      "Average loss at step 300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 6, 5, 3, 5, 5, 5, 1, 4, 1, 4, 1, 2, 5, 2, 3, 2, 3, 6, 3, 2, 3, 4, 7, 4, 1, 4]\n",
      "Average loss at step 400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 2, 5, 3, 3, 5, 3, 1, 4, 4, 1, 7, 2, 5, 5, 2, 6, 3, 6, 6, 3, 5, 4, 7, 7, 4, 4]\n",
      "Average loss at step 500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 2, 6, 3, 3, 5, 3, 5, 4, 7, 4, 4, 4, 5, 6, 3, 5, 3, 6, 5, 2, 6, 2, 7, 4, 1, 7, 1]\n",
      "Average loss at step 600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 6, 2, 3, 5, 5, 5, 3, 7, 4, 1, 4, 4, 6, 3, 2, 3, 5, 5, 2, 3, 2, 6, 4, 1, 4, 1, 7]\n",
      "Average loss at step 700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 7, 7, 7, 4, 1, 6, 6, 6, 5, 2, 5, 5, 5, 6, 3, 4, 4, 4, 7, 4]\n",
      "Average loss at step 800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 4, 7, 4, 3, 6, 5, 6, 5, 2, 5, 6, 5, 6, 1, 4, 7, 4, 7]\n",
      "Average loss at step 900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 2, 5, 3, 3, 5, 3, 1, 7, 4, 4, 4, 2, 6, 5, 3, 5, 3, 5, 6, 2, 6, 4, 4, 7, 1, 7]\n",
      "Average loss at step 1000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 2, 3, 3, 5, 5, 3, 4, 4, 1, 1, 7, 5, 5, 2, 2, 6, 6, 6, 3, 3, 5, 7, 7, 4, 4, 4]\n",
      "Average loss at step 1100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 5, 5, 5, 5, 2, 6, 6, 6, 6, 1, 7, 7, 7, 7]\n",
      "Average loss at step 1200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 6, 5, 3, 3, 3, 5, 1, 7, 7, 7, 4, 2, 6, 6, 6, 3, 3, 5, 5, 5, 2, 4, 4, 4, 4, 1]\n",
      "Average loss at step 1300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 6, 5, 3, 3, 3, 5, 4, 7, 4, 7, 1, 3, 6, 5, 6, 2, 2, 5, 6, 5, 3, 1, 4, 7, 4, 4]\n",
      "Average loss at step 1400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 2, 2, 3, 5, 3, 3, 3, 4, 1, 7, 7, 7, 5, 2, 6, 6, 6, 6, 3, 5, 5, 5, 7, 4, 4, 4, 4]\n",
      "Average loss at step 1500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 2, 2, 5, 3, 5, 3, 3, 1, 4, 1, 7, 4, 2, 5, 2, 6, 5, 3, 6, 3, 5, 6, 4, 7, 4, 4, 7]\n",
      "Average loss at step 1600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 2, 5, 5, 3, 3, 3, 4, 4, 7, 7, 7, 3, 3, 6, 6, 6, 2, 2, 5, 5, 5, 1, 1, 4, 4, 4]\n",
      "Average loss at step 1700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 6, 2, 3, 5, 5, 5, 3, 4, 1, 4, 1, 7, 5, 2, 3, 2, 6, 6, 3, 2, 3, 5, 7, 4, 1, 4, 4]\n",
      "Average loss at step 1800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 1, 4, 4, 4, 7, 2, 5, 5, 5, 6, 3, 6, 6, 6, 5, 4, 7, 7, 7, 4]\n",
      "Average loss at step 1900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 6, 6, 3, 5, 3, 5, 5, 7, 1, 7, 1, 4, 6, 2, 6, 2, 3, 5, 3, 5, 3, 2, 4, 4, 4, 4, 1]\n",
      "Average loss at step 2000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 2, 6, 3, 3, 5, 3, 5, 7, 4, 4, 4, 1, 6, 5, 3, 5, 2, 5, 6, 2, 6, 3, 4, 7, 1, 7, 4]\n",
      "Average loss at step 2100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 4, 7, 7, 4, 3, 5, 6, 6, 5, 2, 6, 5, 5, 6, 1, 7, 4, 4, 7]\n",
      "Average loss at step 2200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 6, 3, 3, 3, 5, 5, 7, 4, 4, 1, 4, 6, 5, 5, 2, 3, 5, 6, 6, 3, 2, 4, 7, 7, 4, 1]\n",
      "Average loss at step 2300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 2, 2, 3, 3, 5, 3, 3, 4, 7, 4, 7, 4, 5, 6, 3, 6, 5, 6, 5, 2, 5, 6, 7, 4, 1, 4, 7]\n",
      "Average loss at step 2400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 6, 3, 3, 5, 5, 5, 7, 4, 1, 4, 4, 6, 5, 2, 3, 3, 5, 6, 3, 2, 2, 4, 7, 4, 1, 1]\n",
      "Average loss at step 2500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 6, 2, 5, 5, 3, 5, 3, 4, 1, 7, 4, 7, 3, 2, 6, 3, 6, 2, 3, 5, 2, 5, 1, 4, 4, 1, 4]\n",
      "Average loss at step 2600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 6, 2, 3, 5, 3, 5, 3, 4, 1, 4, 1, 4, 5, 2, 5, 2, 5, 6, 3, 6, 3, 6, 7, 4, 7, 4, 7]\n",
      "Average loss at step 2700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 2, 5, 3, 3, 5, 3, 1, 4, 7, 4, 4, 2, 5, 6, 3, 5, 3, 6, 5, 2, 6, 4, 7, 4, 1, 7]\n",
      "Average loss at step 2800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 6, 2, 3, 5, 5, 5, 3, 4, 1, 4, 4, 4, 5, 2, 3, 3, 5, 6, 3, 2, 2, 6, 7, 4, 1, 1, 7]\n",
      "Average loss at step 2900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 2, 2, 3, 5, 3, 3, 3, 4, 4, 4, 7, 4, 5, 3, 5, 6, 5, 6, 2, 6, 5, 6, 7, 1, 7, 4, 7]\n",
      "Average loss at step 3000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 6, 2, 3, 5, 5, 5, 3, 4, 4, 1, 1, 4, 5, 3, 2, 2, 5, 6, 2, 3, 3, 6, 7, 1, 4, 4, 7]\n",
      "Average loss at step 3100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 4, 4, 7, 4, 4, 5, 5, 6, 5, 3, 6, 6, 5, 6, 2, 7, 7, 4, 7, 1]\n",
      "Average loss at step 3200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 1, 7, 1, 1, 7, 2, 6, 2, 2, 6, 3, 5, 3, 3, 5, 4, 4, 4, 4, 4]\n",
      "Average loss at step 3300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 6, 3, 3, 5, 5, 5, 7, 7, 1, 1, 4, 6, 6, 2, 2, 3, 5, 5, 3, 3, 2, 4, 4, 4, 4, 1]\n",
      "Average loss at step 3400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 6, 2, 3, 5, 3, 5, 3, 4, 4, 7, 1, 4, 5, 3, 6, 2, 5, 6, 2, 5, 3, 6, 7, 1, 4, 4, 7]\n",
      "Average loss at step 3500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 2, 5, 3, 3, 5, 3, 1, 4, 7, 4, 7, 2, 5, 6, 3, 6, 3, 6, 5, 2, 5, 4, 7, 4, 1, 4]\n",
      "Average loss at step 3600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 2, 3, 3, 5, 5, 3, 4, 7, 1, 4, 7, 5, 6, 2, 3, 6, 6, 5, 3, 2, 5, 7, 4, 4, 1, 4]\n",
      "Average loss at step 3700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 6, 6, 3, 5, 3, 5, 5, 7, 1, 7, 1, 4, 6, 2, 6, 2, 3, 5, 3, 5, 3, 2, 4, 4, 4, 4, 1]\n",
      "Average loss at step 3800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 1, 7, 1, 4, 7, 2, 6, 2, 3, 6, 3, 5, 3, 2, 5, 4, 4, 4, 1, 4]\n",
      "Average loss at step 3900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 6, 5, 3, 3, 3, 5, 4, 4, 4, 4, 4, 3, 5, 5, 5, 3, 2, 6, 6, 6, 2, 1, 7, 7, 7, 1]\n",
      "Average loss at step 4000: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 4, 4, 7, 7, 1, 5, 5, 6, 6, 2, 6, 6, 5, 5, 3, 7, 7, 4, 4, 4]\n",
      "Average loss at step 4100: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 4, 4, 4, 1, 4, 3, 5, 3, 2, 5, 2, 6, 2, 3, 6, 1, 7, 1, 4, 7]\n",
      "Average loss at step 4200: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 4, 7, 1, 1, 4, 3, 6, 2, 2, 5, 2, 5, 3, 3, 6, 1, 4, 4, 4, 7]\n",
      "Average loss at step 4300: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 6, 5, 3, 5, 5, 5, 1, 4, 4, 4, 4, 2, 5, 3, 3, 3, 3, 6, 2, 2, 2, 4, 7, 1, 1, 1]\n",
      "Average loss at step 4400: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 2, 5, 3, 5, 5, 3, 4, 4, 4, 1, 7, 3, 5, 3, 2, 6, 2, 6, 2, 3, 5, 1, 7, 1, 4, 4]\n",
      "Average loss at step 4500: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 6, 5, 3, 3, 5, 5, 4, 4, 7, 4, 4, 3, 5, 6, 3, 3, 2, 6, 5, 2, 2, 1, 7, 4, 1, 1]\n",
      "Average loss at step 4600: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 6, 3, 3, 3, 5, 5, 7, 7, 7, 1, 4, 6, 6, 6, 2, 3, 5, 5, 5, 3, 2, 4, 4, 4, 4, 1]\n",
      "Average loss at step 4700: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 2, 3, 3, 3, 5, 3, 7, 7, 7, 4, 4, 6, 6, 6, 3, 5, 5, 5, 5, 2, 6, 4, 4, 4, 1, 7]\n",
      "Average loss at step 4800: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 6, 2, 3, 5, 5, 5, 3, 7, 1, 1, 4, 4, 6, 2, 2, 3, 5, 5, 3, 3, 2, 6, 4, 4, 4, 1, 7]\n",
      "Average loss at step 4900: 0.000000 learning rate: 10.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 7, 7, 4, 7, 5, 6, 6, 5, 6, 6, 5, 5, 6, 5, 7, 4, 4, 7, 4]\n",
      "Average loss at step 5000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 2, 2, 3, 5, 5, 3, 3, 4, 4, 1, 4, 4, 5, 3, 2, 5, 5, 6, 2, 3, 6, 6, 7, 1, 4, 7, 7]\n",
      "Average loss at step 5100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 7, 7, 4, 3, 6, 6, 6, 5, 2, 5, 5, 5, 6, 1, 4, 4, 4, 7]\n",
      "Average loss at step 5200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 6, 3, 3, 3, 5, 5, 7, 7, 7, 4, 4, 6, 6, 6, 3, 3, 5, 5, 5, 2, 2, 4, 4, 4, 1, 1]\n",
      "Average loss at step 5300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 2, 5, 5, 3, 3, 3, 1, 4, 7, 4, 7, 2, 3, 6, 5, 6, 3, 2, 5, 6, 5, 4, 1, 4, 7, 4]\n",
      "Average loss at step 5400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 6, 2, 5, 5, 3, 5, 3, 4, 1, 7, 4, 7, 3, 2, 6, 3, 6, 2, 3, 5, 2, 5, 1, 4, 4, 1, 4]\n",
      "Average loss at step 5500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 6, 5, 3, 3, 3, 5, 4, 7, 7, 7, 4, 3, 6, 6, 6, 3, 2, 5, 5, 5, 2, 1, 4, 4, 4, 1]\n",
      "Average loss at step 5600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 6, 2, 5, 5, 3, 5, 3, 4, 1, 4, 4, 4, 3, 2, 5, 3, 5, 2, 3, 6, 2, 6, 1, 4, 7, 1, 7]\n",
      "Average loss at step 5700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 7, 4, 7, 3, 6, 6, 5, 6, 2, 5, 5, 6, 5, 1, 4, 4, 7, 4]\n",
      "Average loss at step 5800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 6, 2, 6, 3, 5, 5, 3, 5, 4, 4, 4, 7, 1, 5, 3, 3, 6, 2, 6, 2, 2, 5, 3, 7, 1, 1, 4, 4]\n",
      "Average loss at step 5900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 6, 5, 5, 3, 3, 5, 1, 1, 4, 7, 1, 2, 2, 5, 6, 2, 3, 3, 6, 5, 3, 4, 4, 7, 4, 4]\n",
      "Average loss at step 6000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 6, 2, 5, 3, 3, 5, 3, 4, 4, 4, 4, 7, 3, 5, 5, 3, 6, 2, 6, 6, 2, 5, 1, 7, 7, 1, 4]\n",
      "Average loss at step 6100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 2, 6, 3, 5, 3, 3, 5, 4, 1, 7, 7, 4, 5, 2, 6, 6, 3, 6, 3, 5, 5, 2, 7, 4, 4, 4, 1]\n",
      "Average loss at step 6200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 6, 6, 5, 3, 5, 5, 5, 1, 4, 4, 4, 4, 2, 5, 3, 3, 3, 3, 6, 2, 2, 2, 4, 7, 1, 1, 1]\n",
      "Average loss at step 6300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 6, 2, 5, 5, 5, 5, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 5, 2, 2, 2, 2, 6, 1, 1, 1, 1, 7]\n",
      "Average loss at step 6400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 1, 7, 4, 7, 7, 2, 6, 5, 6, 6, 3, 5, 6, 5, 5, 4, 4, 7, 4, 4]\n",
      "Average loss at step 6500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 6, 2, 2, 5, 3, 5, 3, 3, 4, 7, 1, 4, 7, 3, 6, 2, 5, 6, 2, 5, 3, 6, 5, 1, 4, 4, 7, 4]\n",
      "Average loss at step 6600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 6, 5, 5, 3, 3, 5, 4, 1, 4, 4, 1, 3, 2, 5, 5, 2, 2, 3, 6, 6, 3, 1, 4, 7, 7, 4]\n",
      "Average loss at step 6700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 2, 6, 3, 3, 5, 3, 5, 7, 7, 4, 4, 4, 6, 6, 3, 5, 3, 5, 5, 2, 6, 2, 4, 4, 1, 7, 1]\n",
      "Average loss at step 6800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 7, 4, 4, 7, 1, 6, 5, 5, 6, 2, 5, 6, 6, 5, 3, 4, 7, 7, 4, 4]\n",
      "Average loss at step 6900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 2, 5, 5, 5, 3, 3, 1, 4, 1, 4, 7, 2, 3, 2, 5, 6, 3, 2, 3, 6, 5, 4, 1, 4, 7, 4]\n",
      "Average loss at step 7000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 1, 4, 1, 1, 4, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 4, 1, 4, 4, 1]\n",
      "Average loss at step 7100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 6, 5, 5, 5, 3, 5, 1, 4, 1, 4, 4, 2, 3, 2, 5, 3, 3, 2, 3, 6, 2, 4, 1, 4, 7, 1]\n",
      "Average loss at step 7200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 6, 3, 3, 5, 5, 5, 4, 7, 4, 4, 1, 5, 6, 3, 3, 2, 6, 5, 2, 2, 3, 7, 4, 1, 1, 4]\n",
      "Average loss at step 7300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 2, 2, 3, 3, 5, 3, 3, 7, 7, 1, 7, 7, 6, 6, 2, 6, 6, 5, 5, 3, 5, 5, 4, 4, 4, 4, 4]\n",
      "Average loss at step 7400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 7, 7, 4, 4, 5, 6, 6, 5, 5, 6, 5, 5, 6, 6, 7, 4, 4, 7, 7]\n",
      "Average loss at step 7500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 6, 6, 5, 5, 3, 5, 5, 1, 4, 4, 1, 1, 2, 3, 5, 2, 2, 3, 2, 6, 3, 3, 4, 1, 7, 4, 4]\n",
      "Average loss at step 7600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 6, 2, 5, 5, 3, 5, 3, 4, 1, 7, 1, 4, 3, 2, 6, 2, 5, 2, 3, 5, 3, 6, 1, 4, 4, 4, 7]\n",
      "Average loss at step 7700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 7, 4, 7, 7, 4, 6, 5, 6, 6, 3, 5, 6, 5, 5, 2, 4, 7, 4, 4, 1]\n",
      "Average loss at step 7800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 4, 7, 4, 3, 6, 5, 6, 5, 2, 5, 6, 5, 6, 1, 4, 7, 4, 7]\n",
      "Average loss at step 7900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 6, 5, 5, 5, 3, 5, 4, 4, 4, 7, 1, 3, 3, 3, 6, 2, 2, 2, 2, 5, 3, 1, 1, 1, 4, 4]\n",
      "Average loss at step 8000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 6, 2, 2, 6, 3, 5, 3, 3, 5, 7, 4, 4, 4, 4, 6, 3, 5, 5, 3, 5, 2, 6, 6, 2, 4, 1, 7, 7, 1]\n",
      "Average loss at step 8100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 2, 5, 5, 5, 3, 3, 4, 4, 1, 7, 4, 3, 3, 2, 6, 5, 2, 2, 3, 5, 6, 1, 1, 4, 4, 7]\n",
      "Average loss at step 8200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 6, 3, 3, 5, 5, 5, 7, 7, 1, 1, 1, 6, 6, 2, 2, 2, 5, 5, 3, 3, 3, 4, 4, 4, 4, 4]\n",
      "Average loss at step 8300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 6, 2, 5, 5, 5, 5, 3, 1, 1, 1, 4, 7, 2, 2, 2, 3, 6, 3, 3, 3, 2, 5, 4, 4, 4, 1, 4]\n",
      "Average loss at step 8400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 2, 5, 5, 5, 3, 3, 4, 1, 4, 4, 7, 3, 2, 3, 5, 6, 2, 3, 2, 6, 5, 1, 4, 1, 7, 4]\n",
      "Average loss at step 8500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 7, 4, 4, 3, 6, 6, 5, 5, 2, 5, 5, 6, 6, 1, 4, 4, 7, 7]\n",
      "Average loss at step 8600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 2, 6, 5, 5, 5, 3, 5, 1, 1, 1, 4, 1, 2, 2, 2, 5, 2, 3, 3, 3, 6, 3, 4, 4, 4, 7, 4]\n",
      "Average loss at step 8700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 4, 1, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 4, 4, 4, 1, 4]\n",
      "Average loss at step 8800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 7, 4, 7, 7, 1, 6, 5, 6, 6, 2, 5, 6, 5, 5, 3, 4, 7, 4, 4, 4]\n",
      "Average loss at step 8900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 2, 5, 5, 3, 3, 3, 4, 1, 7, 7, 4, 3, 2, 6, 6, 5, 2, 3, 5, 5, 6, 1, 4, 4, 4, 7]\n",
      "Average loss at step 9000: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 2, 3, 3, 3, 5, 3, 4, 7, 4, 1, 7, 5, 6, 5, 2, 6, 6, 5, 6, 3, 5, 7, 4, 7, 4, 4]\n",
      "Average loss at step 9100: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 7, 7, 4, 7, 1, 6, 6, 5, 6, 2, 5, 5, 6, 5, 3, 4, 4, 7, 4, 4]\n",
      "Average loss at step 9200: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 6, 5, 3, 3, 3, 5, 1, 7, 4, 4, 4, 2, 6, 5, 5, 3, 3, 5, 6, 6, 2, 4, 4, 7, 7, 1]\n",
      "Average loss at step 9300: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 6, 6, 2, 3, 3, 5, 5, 3, 7, 7, 4, 4, 4, 6, 6, 3, 3, 5, 5, 5, 2, 2, 6, 4, 4, 1, 1, 7]\n",
      "Average loss at step 9400: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 7, 4, 4, 5, 5, 6, 5, 5, 6, 6, 5, 6, 6, 7, 7, 4, 7, 7]\n",
      "Average loss at step 9500: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 2, 2, 2, 2, 5, 3, 3, 3, 3, 4, 7, 4, 4, 4, 3, 6, 5, 5, 5, 2, 5, 6, 6, 6, 1, 4, 7, 7, 7]\n",
      "Average loss at step 9600: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 6, 3, 3, 3, 3, 5, 4, 4, 7, 4, 1, 5, 5, 6, 5, 2, 6, 6, 5, 6, 3, 7, 7, 4, 7, 4]\n",
      "Average loss at step 9700: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 6, 2, 3, 3, 3, 5, 3, 4, 4, 7, 4, 4, 5, 5, 6, 3, 5, 6, 6, 5, 2, 6, 7, 7, 4, 1, 7]\n",
      "Average loss at step 9800: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4]\n",
      "Average loss at step 9900: 0.000000 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.00\n",
      "[6, 6, 2, 2, 6, 5, 5, 3, 3, 5, 4, 1, 4, 4, 4, 3, 2, 5, 5, 3, 2, 3, 6, 6, 2, 1, 4, 7, 7, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7ff2fbbc1f10>> ignored\n",
      "Exception AssertionError: AssertionError() in <generator object get_controller at 0x7ff2fc5b6fa0> ignored\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "summary_frequency = 100\n",
    "\n",
    "unrollings_adjuster = partial(adjust_sequence_to_length, padder=predict)\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "\n",
    "merged_summaries = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', sess.graph_def)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "# saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "\n",
    "\n",
    "print('Initialized')\n",
    "mean_loss = 0\n",
    "for step in range(num_epochs):\n",
    "\n",
    "    batches = data_gen.next_batch(unrollings_adjuster=unrollings_adjuster, num_unrollings=n_unrollings+1)\n",
    "    batches_encoded = data_gen.encode_batches(batches,n_unrollings+1)\n",
    "    feed_dict = dict()\n",
    "    # fill data into input placeholders\n",
    "    for i in range(n_unrollings+1):\n",
    "        feed_dict[train_data[i]] = batches_encoded[i]\n",
    "\n",
    "\n",
    "    _, l, prplx, predictions, lr, summaries_str = sess.run([optimizer, loss, perplexity, train_prediction, learning_rate, merged_summaries], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    \n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        \n",
    "        mean_loss = 0\n",
    "        \n",
    "        \n",
    "#         labels = np.concatenate(list(batches_encoded)[1:])\n",
    "#         perplexity = np.exp(logprob(predictions, labels))\n",
    "        \n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            print('Minibatch perplexity: %.2f' % float(prplx))\n",
    "            writer.add_summary(summaries_str, step)\n",
    "#             writer.add_summary(perplexity_summary, step)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#         pr = train_prediction.eval(feed_dict)\n",
    "        pr = predictions\n",
    "        print map(lambda w: data_gen.id2val(data_gen.weighted_pick(w)), pr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([1,2,3], 4, 0), predict([1,2,3], 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7, 6, 5, 4, 3, 4, 1], [7, 6, 5, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([7,6,5], 4, 0), predict([7,6,5], 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 7, 6, 5, 1], [1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([1,2,3,7], 3, 0), predict([1,2,3,4], 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = data_gen.next_batch(unrollings_adjuster=unrollings_adjuster, num_unrollings=n_unrollings+1)\n",
    "# batches_encoded = data_gen.encode_batches(batches,n_unrollings+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen.encode_batches(batches,n_unrollings+1+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen.encode_batches(batches,n_unrollings+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_unrollings+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen = BatchGenerator(2,4)\n",
    "batches = data_gen.next_batch()\n",
    "batches\n",
    "# batches_encoded = data_gen.encode_batches(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]], shape=[2, 3], name='a')\n",
    "b = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "\n",
    "print a.eval()\n",
    "print b.eval()\n",
    "print c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]], shape=[2, 3], name='a')\n",
    "b = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]], shape=[3, 2], name='b')\n",
    "d = tf.constant([[1.0, 2.], [3., 4.]], name='d')\n",
    "c = tf.matmul(a, b)+d\n",
    "\n",
    "\n",
    "\n",
    "print a.eval()\n",
    "print b.eval()\n",
    "print d.eval()\n",
    "print c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                     [[3, 3, 3], [4, 4, 4]],\n",
    "                     [[5, 5, 5], [6, 6, 6]]])\n",
    "i1 = tf.slice(input, [1, 0, 0], [1, 1, 3])\n",
    "# tf.slice(input, [1, 0, 0], [1, 2, 3]) ==> [[[3, 3, 3],\n",
    "#                                             [4, 4, 4]]]\n",
    "# tf.slice(input, [1, 0, 0], [2, 1, 3]) ==> [[[3, 3, 3]],\n",
    "#                                            [[5, 5, 5]]]\n",
    "print input, i1\n",
    "print input.eval()\n",
    "print\n",
    "print i1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = tf.constant([[1, 1, 2],\n",
    "                     [3, 3, 4],\n",
    "                     [5, 5, 6]])\n",
    "# i2 = tf.slice(input, [0, 0], [3, 2])\n",
    "s = input.get_shape()\n",
    "i2 = tf.slice(input, [0, 0], np.array(s.as_list())-np.array([0,2]))\n",
    "# tf.slice(input, [1, 0, 0], [1, 2, 3]) ==> [[[3, 3, 3],\n",
    "#                                             [4, 4, 4]]]\n",
    "# tf.slice(input, [1, 0, 0], [2, 1, 3]) ==> [[[3, 3, 3]],\n",
    "#                                            [[5, 5, 5]]]\n",
    "print input, i2\n",
    "print input.eval()\n",
    "print\n",
    "print i2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = input.get_shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ini_steps = [2,3,1]\n",
    "jrn_ids = map(data_gen.val2id, ini_steps)\n",
    "jrn_1h = map(data_gen.encode_1h, jrn_ids)\n",
    "\n",
    "\n",
    "# for _ in range(7-len(jrn_ids)):\n",
    "for _ in range(10-len(jrn_ids)):\n",
    "    sample_inputs, model = get_predictor(len(jrn_ids))\n",
    "\n",
    "    dick ={}\n",
    "    for i, s in zip(sample_inputs, jrn_1h):                        \n",
    "        dick[i]=s\n",
    "\n",
    "    prediction = model.eval(dick)\n",
    "\n",
    "    pred_step_id = data_gen.weighted_pick(prediction[-1].ravel())                                          \n",
    "    jrn_ids.append(pred_step_id)                 \n",
    "\n",
    "    pred_step_1h = data_gen.encode_1h(pred_step_id)                    \n",
    "    jrn_1h.append(pred_step_1h)                    \n",
    "\n",
    "\n",
    "\n",
    "print map(lambda w: data_gen.id2val(w), jrn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = [1,2,3]\n",
    "print np.pad(arr,pad_width=3, mode='reflect')\n",
    "print np.pad(arr,pad_width=3, mode='symmetric')\n",
    "print np.pad(arr,pad_width=3, mode='wrap')\n",
    "print np.pad(arr,pad_width=3, mode='edge')\n",
    "print np.pad(arr,pad_width=3, mode='constant',constant_values=9)\n",
    "print np.pad(arr,pad_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(n_unrollings+1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:n_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    train_labels_concat = tf.concat(0, train_labels)\n",
    "    \n",
    "    #social Input data\n",
    "    social_input = tf.placeholder(tf.float32, shape=[batch_size, social_vocabulary_size])\n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as rnn_scope:\n",
    "    \n",
    "        \n",
    "    \n",
    "        # RNN struct\n",
    "        cell = rnn_cell.LSTMCell(num_units=n_hidden, input_size=vocabulary_size)     \n",
    "        outputs, states = rnn.rnn(cell, train_inputs, dtype=tf.float32) \n",
    "        outputs_concat = tf.concat(0, outputs)#tf.reshape(tf.concat(1, outputs), (-1, n_hidden))\n",
    "\n",
    "        # Classifier.\n",
    "        W_hy = tf.get_variable(\"W_hy\", [n_hidden, vocabulary_size])\n",
    "        b_hy = tf.get_variable(\"b_hy\", [vocabulary_size])\n",
    "        \n",
    "        \n",
    "        W_social = tf.get_variable(\"W_social\", [n_hidden, social_vocabulary_size])\n",
    "        b_social = tf.get_variable(\"b_social\", [social_vocabulary_size])\n",
    "        \n",
    "        logits = tf.matmul(outputs_concat, W_hy) + b_hy\n",
    "\n",
    "        # Loss func\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, train_labels_concat))\n",
    "\n",
    "        \n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name='global_step')\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        \n",
    "        #perplexity\n",
    "#         predictions[predictions < 1e-10] = 1e-10\n",
    "#         return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]        \n",
    "        perplexity = tf.reduce_sum(tf.mul(train_labels_concat, -tf.log(train_prediction)))/train_labels_concat.get_shape()[0].value\n",
    "        \n",
    "        loss_summary = tf.scalar_summary(\"loss\", loss)\n",
    "        learning_rate_summary = tf.scalar_summary(\"learning_rate\", learning_rate)\n",
    "        perplexity_summary = tf.scalar_summary(\"perplexity_summary\", perplexity)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
