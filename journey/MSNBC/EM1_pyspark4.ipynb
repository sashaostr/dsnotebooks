{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "import re\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "from functools import reduce\n",
    "\n",
    "\"\"\"\n",
    "sources: \n",
    "http://windowoffice.tumblr.com/post/33548509/logsum-underflow-trick-re-discovery\n",
    "https://facwiki.cs.byu.edu/nlp/index.php/Log_Domain_Computations\n",
    "https://mikelove.wordpress.com/2011/06/06/log-probabilities-trick/\n",
    "\"\"\"\n",
    "def log_add(logx,logy):\n",
    "\n",
    "    if (logy > logx):\n",
    "        logy,logx = logx,logy\n",
    "        \n",
    "    negdiff = logy - logx\n",
    "\n",
    "    if negdiff < -20:\n",
    "        return logx\n",
    "\n",
    "    return logx + log(1.0 + exp(negdiff))\n",
    "\n",
    "def sum_logs(logs):\n",
    "    \"\"\"\n",
    "    return the log of total values\n",
    "    >>> exp(sum_logs([log(0.1),log(0.4),log(0.6)]))\n",
    "    1.1\n",
    "    \"\"\"\n",
    "    return reduce(log_add,logs) \n",
    "\n",
    "def normalize_logs(logs):\n",
    "    \"\"\"\n",
    "    convert the logs to normalized probablities\n",
    "    >>>normalize_logs(array([-1000,-1000,-990]))\n",
    "    array([  4.53958078e-05,   4.53958078e-05,   9.99909208e-01])\n",
    "    \"\"\"\n",
    "    log_total = sum_logs(logs)\n",
    "    logprobs = logs-log_total\n",
    "    probs = np.exp(logprobs)\n",
    "    return probs\n",
    "\n",
    "def pick_random_points(logprobs,num_points):\n",
    "    \"\"\"\n",
    "    picks random entries according to the logprobs vector\n",
    "    >>>np.random.seed(24)\n",
    "    >>>pick_random_points(array([log(0.1),log(0.1),log(0.5),log(0.3)]),num_points=10)\n",
    "    array([3, 2, 3, 2, 2, 3, 3, 2, 1, 2])\n",
    "    \"\"\"\n",
    "    probs = normalize_logs(logprobs)\n",
    "    xk = np.arange(len(probs))\n",
    "    custm = stats.rv_discrete(name='custm', values=(xk, probs))\n",
    "    return custm.rvs(size=num_points)\n",
    "\n",
    "def logprobs_to_normprobs(logprobs):\n",
    "    probs = np.exp(logprobs)\n",
    "    norm_prbs = probs/np.sum(probs)\n",
    "    return norm_prbs\n",
    "\n",
    "def logprobs_to_normprobs_safe(logprobs):\n",
    "#     this version uses log addition\n",
    "    logprob_total = sum_logs(logprobs)\n",
    "    norm_logprbs = logprobs-logprob_total\n",
    "    norm_prbs = np.exp(norm_logprbs)\n",
    "    return norm_prbs\n",
    "\n",
    "def calc_exp_log_prob(probs,logprobs):\n",
    "    return np.sum([0 if prob<=0 else prob*logprob for prob,logprob in zip(probs,logprobs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"app\").set(\"spark.executor.memory\", \"1500M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END_STATE = sc.broadcast(100)\n",
    "END_STATE_NAME = sc.broadcast(\"End\")\n",
    "START_STATE = sc.broadcast(0)\n",
    "STATE_NAMES = sc.broadcast(['Start','frontpage','news','tech','local','opinion','on-air','misc','weather','msn-news','health','living','business','msn-sports','sports','summary','bbs','travel'])\n",
    "# num_partitions = 3\n",
    "# num_clusters = sc.broadcast(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_traj(traj):\n",
    "    return [START_STATE.value]+list(traj)+[END_STATE.value,END_STATE.value]\n",
    "def unpad_traj(traj):\n",
    "    return traj[1:-2]\n",
    "def state_to_name(state):\n",
    "    if state==END_STATE:\n",
    "        return END_STATE_NAME\n",
    "    else:\n",
    "        return STATE_NAMES[state]\n",
    "def traj_to_namedstate(traj):\n",
    "    return map(state_to_name,traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EA_SMOOTH_TRANSITION = 0.1\n",
    "\n",
    "def create_smooth_transitions(states=[]):\n",
    "    return {(i,j):EA_SMOOTH_TRANSITION for i in states for j in states}\n",
    "\n",
    "def create_smooth_transitions_list(states=[]):\n",
    "    return [((i,j),EA_SMOOTH_TRANSITION) for i in states for j in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emit_ind_trans_prop(ind__traj__clsp_rob):\n",
    "    '''\n",
    "    >>> ind__traj__clsp_rob = (0, ([0, 1, 1, 100, 100], array([ 0.50891031,  0.26625345,  0.22483623]))))\n",
    "    >>> emit_ind_trans_prop(ind__traj__clsp_rob)\n",
    "    [((0, (0, 1)), 0.50891030999999998), ((0, (1, 1)), 0.50891030999999998), ((0, (1, 100)), 0.50891030999999998), ((0, (100, 100)), 0.50891030999999998), ((1, (0, 1)), 0.26625345), ((1, (1, 1)), 0.26625345), ((1, (1, 100)), 0.26625345), ((1, (100, 100)), 0.26625345), ((2, (0, 1)), 0.22483623), ((2, (1, 1)), 0.22483623), ((2, (1, 100)), 0.22483623), ((2, (100, 100)), 0.22483623)]\n",
    "    '''\n",
    "    index = ind__traj__clsp_rob[0]\n",
    "    traj = ind__traj__clsp_rob[1][0]\n",
    "    clusters_probs = ind__traj__clsp_rob[1][1]\n",
    "    \n",
    "    cl_step_propbs = []\n",
    "    \n",
    "    trans = zip(traj[:-1],traj[1:])\n",
    "    for c_ind, cls_p in enumerate(clusters_probs):\n",
    "        for step in trans:        \n",
    "            cl_step_propbs.append(((c_ind, step), (cls_p, index)))\n",
    "#             ini_p = smooth_trans_mtrx[step]\n",
    "#             cl_step_propbs.append(((c_ind, step), (ini_p + cls_p, index)))\n",
    "            #             yield ((c_ind, step), cls_p)\n",
    "        \n",
    "    return cl_step_propbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_list_based_on_another(base, dependent):\n",
    "    '''\n",
    "    >>> sort_list_based_on_another([0, 2, 1], [0.22, 0.55, 0.66])\n",
    "    ([0, 1, 2], [0.22, 0.66, 0.55])\n",
    "    '''\n",
    "    z = zip(base, dependent)\n",
    "    sz = sorted(z)\n",
    "    return [t[0] for t in sz], [t[1] for t in sz]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def em_train2(list_traj, num_iterations=1, num_clusters=3,num_partitions=3):\n",
    "    \n",
    "    persistedRDDs = []\n",
    "    num_part = num_partitions\n",
    "    \n",
    "    \n",
    "    list_traj = list_traj.partitionBy(num_part).persist(StorageLevel.DISK_ONLY).setName('list_traj')\n",
    "    persistedRDDs.append(list_traj)\n",
    "    \n",
    "    num_traj = list_traj.count()\n",
    "    \n",
    "    # measure_relative_state_size = states prior probs\n",
    "    states = list_traj.flatMap(lambda states: states[1])\n",
    "    states_counts = states.countByValue()\n",
    "    states_counts_sum = sum(states_counts.values())\n",
    "    states_dict = {k:float(v)/float(states_counts_sum)  for k,v in states_counts.iteritems()}\n",
    "#     states = sc.parallelize([k:float(v)/float(states_counts_sum)  for k,v in states_counts.iteritems()])\n",
    "#   [(step, EA_SMOOTH_TRANSITION), ]\n",
    "    smooth_mtrx_list = sc.parallelize(create_smooth_transitions_list(states_dict)).flatMap(lambda t:[((c,t[0]), t[1]) for c in range(num_clusters)]) \\\n",
    "                                                                                  .persist(StorageLevel.MEMORY_AND_DISK) \\\n",
    "                                                                                  .setName('smooth_mtrx_list')\n",
    "                                \n",
    "    \n",
    "    \n",
    "    #init trajectories probs (init_traj_probs\n",
    "    # list_traj_probs = sc.parallelize(np.random.rand(num_traj,3),num_partitions).map(lambda vec: vec/np.sum(vec))\n",
    "    #sasha: probably need to make in list_traj, so no need to join\n",
    "    list_traj_probs = sc.parallelize(xrange(num_traj), num_partitions) \\\n",
    "                                .map(lambda v: np.random.rand(1,num_clusters)[0]) \\\n",
    "                                .map(lambda vec: vec/np.sum(vec)) \\\n",
    "                                .zipWithIndex() \\\n",
    "                                .map(lambda t: (t[1], t[0])) \\\n",
    "                                .partitionBy(num_part).persist(StorageLevel.DISK_ONLY) \\\n",
    "                                .setName('list_traj_probs')\n",
    "                            \n",
    "    persistedRDDs.append(list_traj_probs)\n",
    "    \n",
    "    t = TimeHelper()\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "      \n",
    "        # join trajectories and clusters initial random probs\n",
    "        # ((index), ([traj], [cluster_prob]))\n",
    "        list_traj__traj_probs = list_traj.join(list_traj_probs, num_part) \n",
    "        \n",
    "        # ((cluster, trans), (p, index))\n",
    "        # [((0, (0, 1)), (0.24919889991572008, 0)), ((0, (1, 1)), (0.24919889991572008, 0))]\n",
    "        list_cls_trans__p_index = list_traj__traj_probs.flatMap(emit_ind_trans_prop)\n",
    "        \n",
    "        # ((cluster, trans), p)\n",
    "        # [((0, (0, 1)), 0.24919889991572008), ((0, (1, 1)), 0.24919889991572008)]\n",
    "        list_cls_trans__p = list_cls_trans__p_index.map(lambda t: (t[0],t[1][0]))\n",
    "#         list_cls_trans__p.persist(StorageLevel.MEMORY_AND_DISK)      \n",
    "\n",
    "        # [((cluster, trans), p), ]  \n",
    "        # [((0, (0, 1)), 0.15297377614141211), ((0, (1, 1)), 0.15297377614141211)\n",
    "        list_cls_trans__p = list_cls_trans__p.union(smooth_mtrx_list) \\\n",
    "                                            .partitionBy(num_part*2) \\\n",
    "                                            .persist(StorageLevel.DISK_ONLY) \\\n",
    "                                            .setName('list_cls_trans__p'+str(i))                \n",
    "        persistedRDDs.append(list_cls_trans__p)\n",
    "        \n",
    "        list_traj_probs.unpersist()\n",
    "        #################################  CALC MARKOV   ##############################\n",
    "        \n",
    "        # ((cluster, from), (trans, p))\n",
    "        # [((0, 0), ((0, 3), 0.45565240865632561)), ((0, 3), ((3, 100), 0.45565240865632561))]\n",
    "#         list_cls_from__trans_p = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1],t[1][0])))\n",
    "        list_cls_from__trans_p = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1],t[1])))\n",
    "        \n",
    "        # ((cluster, from), p)\n",
    "        # [((0, 0), 0.45565240865632561), ((0, 3), 0.45565240865632561)]\n",
    "        list_cls_from__p = list_cls_from__trans_p.map(lambda t: (t[0], t[1][1]))\n",
    "        # ((claster, from), summ)\n",
    "        # [((1, 3), 88045.381738379481), ((1, 13), 93509.65969107172), ((0, 100), 428820.63873856084), ((2, 10), 53400.141021192758)]\n",
    "        cls_from__summ = list_cls_from__p.reduceByKey(lambda x,y: x+y) \\\n",
    "                                         .coalesce(num_part)\n",
    "    \n",
    "        # ((cluster,trans), sum)\n",
    "        # [((2, (16, 100)), 452.38469331913268), ((0, (15, 5)), 1081.6800474165561), ((0, (8, 8)), 142241.31289751496)]\n",
    "#         cls_trans__summ = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1]), t[1][0])) \\\n",
    "        cls_trans__summ = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1]), t[1])) \\\n",
    "                                           .reduceByKey(lambda x,y: x+y) \\\n",
    "                                           .coalesce(num_part)\n",
    "        \n",
    "        #  ((cluster, from), ((trans, trans_sum), from_summ))\n",
    "        # [((2, 12), (((12, 10), 1019.2083959882245), 113267.37822605985)), ((2, 12), (((12, 2), 3666.330047976217), 113267.37822605985))\n",
    "        cls_from__transsum_summ = cls_trans__summ.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1], t[1]))) \\\n",
    "                                                .join(cls_from__summ, num_part)\n",
    "            \n",
    "        # [((2, (0, 4)), 0.05168807562712003), ((2, (0, 16)), 0.00042905633895048338), ((2, (0, 6)), 0.16451598030626441)]\n",
    "        markov_models = cls_from__transsum_summ.map(lambda t: ((t[0][0], t[1][0][0]), t[1][0][1]/t[1][1]))\n",
    "                \n",
    "            \n",
    "        #################################  CALC NEW PROBS   ##############################\n",
    "                \n",
    "        # [((2, (16, 100)), ((0.42948178892760447, 264), 0.043625160028027611)), ((2, (16, 100)), ((0.52153298549282001, 262602), 0.043625160028027611))]\n",
    "        list_cls_trans__p_index_mrkp = list_cls_trans__p_index.join(markov_models, num_part)\n",
    "#         list_cls_trans__p_index_mrkp.persist(StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        # ((cluster, index), (trans, logP))\n",
    "        # [((0, 789061), ((11, 7), -5.004076355127463)), ((0, 789061), ((11, 7), -5.004076355127463))]\n",
    "        list_cls_index__trans_logmrkp = list_cls_trans__p_index_mrkp.map(lambda t: ((t[0][0],t[1][0][1]), (t[0][1], math.log(t[1][1]))))\n",
    "\n",
    "        # walk_logprobs\n",
    "        # ((cluster, index), [(trans1, logP1),((trans2, logP2))])\n",
    "        # [ ((0, 784470), [((9, 100), -1.3163148315938333), ((0, 9), -2.717692054383914), ((100, 100), 0.0)])]\n",
    "        list_cls_index__grp_trans_logmrkp = list_cls_index__trans_logmrkp.groupByKey()\n",
    "        \n",
    "        # calc_walk_probs\n",
    "        # (index, (cluster, walk_logprob))\n",
    "        # [(784470, (0, -4.0340068859777469)), (950019, (1, -2.8974930003028816))\n",
    "        cls_index__sumlogmrkp = list_cls_index__grp_trans_logmrkp.map(lambda t: (t[0][1], (t[0][0], sum([tr_logp[1] for tr_logp in t[1]]))))\n",
    "        \n",
    "        # [(index, [(cluster1, walk_logprob1), (cluster2, walk_logprob2), (cluster3, walk_logprob3)], ]      \n",
    "        # [(0, [(0, -3.7289041683043873), (2, -3.7284874541593469), (1, -3.7306682864068987)], (655362, <pyspark.resultiterable.ResultIterable object at 0x7f01bf534790>)]\n",
    "        index__cls_sumlogmrkp = cls_index__sumlogmrkp.groupByKey()\n",
    "        \n",
    "        # cluster_probs = logprobs_to_normprobs_safe\n",
    "        # (index, [p1,p2,p3])\n",
    "        # [(655362, [0.3332682408091523, 0.3329057626978422, 0.333825996493006]), ]\n",
    "        list_traj_probs = index__cls_sumlogmrkp.map(lambda t: (t[0], \\\n",
    "                                    ([c_logp[0] for c_logp in t[1]], \\\n",
    "                                     logprobs_to_normprobs_safe(array([c_logp[1] for c_logp in t[1]]))))) \\\n",
    "                                  .map(lambda t: (t[0], (sort_list_based_on_another(t[1][0], t[1][1]))[1] )) \\\n",
    "                                  .partitionBy(num_part).persist(StorageLevel.DISK_ONLY) \\\n",
    "                                  .setName('list_traj_probs'+str(i))\n",
    "        \n",
    "        \n",
    "        print list_traj_probs.count()\n",
    "        \n",
    "        for prdd in persistedRDDs:\n",
    "            prdd.unpersist()\n",
    "        del persistedRDDs[:]\n",
    "            \n",
    "        t.print_full_time_from_last()\n",
    "        \n",
    "    persistedRDDs.append(list_traj_probs)     \n",
    "    persistedRDDs.append(smooth_mtrx_list)\n",
    "        \n",
    "    list_assignment = list_traj_probs.map(lambda t: (t[0], np.argmax(t[1])))\n",
    "    return markov_models, list_assignment, persistedRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def part_pers(rdd, partitions=3, persist=StorageLevel.MEMORY_AND_DISK, name=''):\n",
    "    rdd = rdd.partitionBy(partitions)\n",
    "    rdd = rdd.persist(persist)\n",
    "    if name != '':\n",
    "        rdd = rdd.setName(name)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TimeHelper:\n",
    "    \n",
    "    def __init__(self, prefix_str=\"time\"):   \n",
    "        self.prefix_str = prefix_str\n",
    "        self.reset()\n",
    "    \n",
    "    def get_time_from_last(self):\n",
    "        now = datetime.datetime.now()\n",
    "        delta = now - self.last\n",
    "        self.last = now        \n",
    "        return delta\n",
    "    \n",
    "    def print_time_from_last(self):\n",
    "        print self.prefix_str + \": \" + str(get_time_from_last())\n",
    "        \n",
    "    def print_full_time_from_last(self):\n",
    "        print self.prefix_str + \": \" + strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + \" \" + str(self.get_time_from_last())\n",
    "        \n",
    "    def reset(self):\n",
    "        self.last=datetime.datetime.now()\n",
    "    \n",
    "def print_count(rdd, name):\n",
    "#     rdd.cache()\n",
    "    rdd.persist(StorageLevel.DISK_ONLY)\n",
    "    print name + \": \" + str(rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MSNBC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msnbc_no_header = sc.textFile(\"/ea/msnbc_no_header.seq\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read trajectories\n",
    "list_trajRDD = msnbc_no_header.map(lambda line: [int(i) for i in line.split()]) \\\n",
    "                           .filter(lambda seq: len(seq) <= 500) \\\n",
    "                           .map(pad_traj) \\\n",
    "                           .zipWithIndex() \\\n",
    "                           .map(lambda t: (t[1], t[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = TimeHelper()\n",
    "th.print_full_time_from_last()\n",
    "markovsRDD, assignmentsRDD, persistedRDDs = em_train2(list_trajRDD, num_iterations=4,num_clusters=6, num_partitions=6)\n",
    "assignments = assignmentsRDD.collect()\n",
    "markovs_by_cls = markovsRDD.map(lambda t: (t[0][0],(t[0][1],t[1]))).groupByKey().collect()\n",
    "\n",
    "for prdd in persistedRDDs:\n",
    "    prdd.unpersist()\n",
    "    \n",
    "th.print_full_time_from_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assignments3 = assignments\n",
    "# markovs_by_cls3 = markovs_by_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_next_step(trans_vec):\n",
    "    next_step_vec = np.random.multinomial(1, trans_vec, size=1)[0]\n",
    "    return np.sum(next_step_vec*range(len(trans_vec)))\n",
    "\n",
    "def generate_test_trajs(trans_mtrx,num_trajs=100,traj_len=10):\n",
    "    trajs = []\n",
    "    for i in range(num_trajs):\n",
    "        test_traj = generate_test_traj(trans_mtrx,traj_len)\n",
    "        trajs.append(test_traj)\n",
    "    return trajs\n",
    "def generate_test_traj(trans_mtrx,traj_len=10):\n",
    "    cur_pos = 0\n",
    "    test_traj = [cur_pos]\n",
    "    for j in range(traj_len):\n",
    "        next_pos = calc_next_step(trans_mtrx[cur_pos])\n",
    "        test_traj.append(next_pos)\n",
    "        cur_pos = next_pos\n",
    "    return array(test_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans_mtrx = [[0.1,0.8,0.1],[0.4,0.2,0.4],[0.1,0.2,0.7]]\n",
    "trans_mtrx = [[0.8,0.1,0.1],[1,0,0],[0.9,0.1,0]]\n",
    "def generate_tarjs_per_cluster(list_trans_mtrx,num_trajs=100,traj_len=20):\n",
    "    trajs = []\n",
    "    labels = []\n",
    "    for index,trans_mtrx in enumerate(list_trans_mtrx):\n",
    "        cur_test_trajs = generate_test_trajs(trans_mtrx,num_trajs,traj_len)\n",
    "        trajs += cur_test_trajs\n",
    "        labels += [index]*num_trajs\n",
    "    \n",
    "    return DataFrame(data={'label':labels,'traj':trajs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_mtrx0 = [[0.8,0.1,0.05,0.05],[0.95,0,0,0.05],[0.9,0.05,0,0.05],[0.9,0.05,0,0.05]]\n",
    "trans_mtrx1 = [[0.1,0.8,0.05,0.05],[0,0.7,0.25,0.05],[0,0.75,0.1,0.15],[0,0.7,0.25,0.05]]\n",
    "trans_mtrx2 = [[0.1,0.15,0.7,0.05],[0.05,0.1,0.8,0.05],[0.2,0.15,0.6,0.05],[0.1,0.15,0.7,0.05]]\n",
    "list_trans_mtrx = [trans_mtrx0,trans_mtrx1,trans_mtrx2]\n",
    "num_trajs=100\n",
    "df_traj = generate_tarjs_per_cluster(list_trans_mtrx,num_trajs=num_trajs,traj_len=40)\n",
    "df_traj = df_traj.iloc[np.random.permutation(len(df_traj))]\n",
    "df_traj = df_traj.reset_index(drop=True)\n",
    "df_traj['is_train']=True\n",
    "df_traj.ix[df_traj.index<int(len(df_traj)*0.2),'is_train']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_clusters = sc.broadcast(3)\n",
    "train_trajs = df_traj[df_traj.is_train==True].traj\n",
    "test_trajs = df_traj[df_traj.is_train==True].traj\n",
    "\n",
    "# train_walks = [traj_to_walk(traj) for traj in train_trajs]\n",
    "# test_walks = [traj_to_walk(traj) for traj in test_trajs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trajs_to_walks' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-229-33b235538418>\"\u001b[1;36m, line \u001b[1;32m2\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_walks = trajs_to_walks(train_trajs,states)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'trajs_to_walks' is not defined\n"
     ]
    }
   ],
   "source": [
    "states = [0,1,2,3]\n",
    "train_walks = trajs_to_walks(train_trajs,states)\n",
    "test_walks = trajs_to_walks(test_trajs,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_trajsRDD = sc.parallelize(train_trajs.tolist(),3) \\\n",
    "                        .map(lambda t: t.tolist()) \\\n",
    "                        .zipWithIndex() \\\n",
    "                        .map(lambda t: (t[1], t[0]))\n",
    "                        \n",
    "    \n",
    "# train_trajsRDD.take(3)\n",
    "# train_trajsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "time: 2015-07-29 11:31:44 0:00:01.759580\n",
      "240\n",
      "time: 2015-07-29 11:31:46 0:00:01.710593\n",
      "240\n",
      "time: 2015-07-29 11:31:48 0:00:01.703535\n",
      "240\n",
      "time: 2015-07-29 11:31:50 0:00:01.701546\n",
      "240\n",
      "time: 2015-07-29 11:31:51 0:00:01.717647\n",
      "240\n",
      "time: 2015-07-29 11:31:53 0:00:01.689329\n",
      "240\n",
      "time: 2015-07-29 11:31:55 0:00:01.713638\n",
      "240\n",
      "time: 2015-07-29 11:31:56 0:00:01.786585\n",
      "240\n",
      "time: 2015-07-29 11:31:58 0:00:01.818904\n",
      "240\n",
      "time: 2015-07-29 11:32:00 0:00:01.762461\n",
      "time: 2015-07-29 11:32:01 0:00:18.321497\n"
     ]
    }
   ],
   "source": [
    "t = TimeHelper()\n",
    "markovsRDD, assignmentsRDD, persistedRDDs = em_train2(train_trajsRDD,num_iterations=10,num_clusters=3,num_partitions=7)\n",
    "assignments = assignmentsRDD.collect()\n",
    "markovs_by_cls = markovsRDD.map(lambda t: (t[0][0],(t[0][1],t[1]))).groupByKey().collect()\n",
    "\n",
    "for prdd in persistedRDDs:\n",
    "    prdd.unpersist()\n",
    "\n",
    "t.print_full_time_from_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {(1, 2): 0.00033967391304347831, (0, 1): 0.10090928464871778, (3, 2): 0.00053078556263269649, (1, 3): 0.071671195652173919, (3, 3): 0.069532908704883239, (3, 0): 0.88163481953290879, (3, 1): 0.048301486199575375, (0, 3): 0.053313916317397167, (0, 2): 0.051893159053775659, (2, 0): 0.91422594142259428, (0, 0): 0.79388363998010936, (2, 3): 0.042538354253835432, (2, 1): 0.042538354253835432, (2, 2): 0.00069735006973500717, (1, 0): 0.92764945652173925, (1, 1): 0.00033967391304347831}, 1: {(0, 1): 0.84889434889434878, (1, 2): 0.23746689368142268, (3, 2): 0.25978473581213307, (0, 0): 0.074938574938574934, (3, 3): 0.049412915851272013, (3, 0): 0.00048923679060665359, (3, 1): 0.69031311154598818, (2, 1): 0.74509477884935149, (1, 1): 0.71467082860385933, (2, 0): 0.00016627868307283005, (1, 3): 0.047814982973893305, (2, 3): 0.15314266711007646, (2, 2): 0.10159627535749916, (1, 0): 4.729474082482029e-05, (0, 3): 0.050368550368550362, (0, 2): 0.025798525798525797}, 2: {(0, 1): 0.15868366643711046, (1, 2): 0.80814747382581309, (3, 2): 0.70326223337507532, (0, 0): 0.11044107512058422, (3, 3): 0.050815558343677285, (3, 0): 0.1072772898366509, (3, 1): 0.13864491844459631, (2, 1): 0.14936440678006188, (0, 2): 0.66350792556843896, (2, 0): 0.20989709443080606, (1, 3): 0.050295857988150956, (2, 3): 0.047972154963756251, (2, 2): 0.59276634382537585, (1, 0): 0.052571688666067769, (0, 3): 0.067367332873866426, (1, 1): 0.0889849795199677}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markovs_dict = {}\n",
    "for c in markovs_by_cls:    \n",
    "    m = {tr_p[0]:tr_p[1] for tr_p in c[1]}\n",
    "    markovs_dict[c[0]] = m\n",
    "markovs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assignments.sort(key=lambda t: t[0])\n",
    "list_assignment = [ass[1] for ass in assignments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_traj[df_traj.is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_assign = range(3)\n",
    "df = DataFrame({'real':array(df_traj[df_traj.is_train==True].label),'assign':list_assignment})\n",
    "real_assign = range(3)\n",
    "df['compare'] = df.apply(lambda x: real_assign[x['real']]==x['assign'],axis=1)\n",
    "len(df[df.compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1, 2), 240)\n"
     ]
    }
   ],
   "source": [
    "# all_combs = itertools.chain(*(itertools.combinations(filter_vars, i) for i in range(1,3)))\n",
    "import itertools\n",
    "assigns = {}\n",
    "for real_assign in itertools.permutations(range(3)):\n",
    "    df['compare'] = df.apply(lambda x: real_assign[x['real']]==x['assign'],axis=1)\n",
    "    num_eq = len(df[df.compare])\n",
    "    assigns[real_assign] = num_eq\n",
    "\n",
    "import operator\n",
    "print max(assigns.iteritems(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "0:00:00.980719\n",
      "1\n",
      "100000\n",
      "0:00:01.326909\n",
      "2\n",
      "100000\n",
      "0:00:01.207190\n",
      "3\n",
      "100000\n",
      "0:00:01.225242\n",
      "4\n",
      "100000\n",
      "0:00:01.192086\n",
      "5\n",
      "100000\n",
      "0:00:01.164828\n",
      "0:00:07.475116\n"
     ]
    }
   ],
   "source": [
    "init = sc.parallelize(xrange(100000), 3)\n",
    "init.cache()\n",
    "gstart = datetime.datetime.now()\n",
    "\n",
    "for i in range(6):\n",
    "    print i\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    init2 = init.map(lambda n: (n, n*3)).partitionBy(3)\n",
    "#     init2.cache()\n",
    "    \n",
    "    init3 = init.map(lambda n: (n, n*2)).partitionBy(3)\n",
    "#     init3.cache()\n",
    "        \n",
    "    init4 = init2.join(init3, 3)\n",
    "#     init4.count()\n",
    "#     init4.cache()\n",
    "    \n",
    "    init = init4.map(lambda n: n[0])\n",
    "#     init.cache()\n",
    "    \n",
    "    print init.count()    \n",
    "    print str(datetime.datetime.now() - start)\n",
    "    \n",
    "init.cache()\n",
    "init.count()\n",
    "print str(datetime.datetime.now() - gstart)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:00:00.000042\n",
      "1\n",
      "0:00:00.000019\n",
      "2\n",
      "0:00:00.000017\n",
      "3\n",
      "0:00:00.000016\n",
      "4\n",
      "0:00:00.000017\n",
      "5\n",
      "0:00:00.000016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = sc.parallelize(xrange(10000000), 3)\n",
    "init.cache()\n",
    "\n",
    "for i in range(6):\n",
    "    print i\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    init2 = init.map(lambda n: (n, n*3))        \n",
    "    init = init2.map(lambda n: n[0])\n",
    "    \n",
    "#     init.cache()\n",
    "    \n",
    "#     print init.count()    \n",
    "    print str(datetime.datetime.now() - start)\n",
    "    \n",
    "init.count()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000000\n",
      "0:00:02.626615\n",
      "1\n",
      "10000000\n",
      "0:00:02.361970\n",
      "2\n",
      "10000000\n",
      "0:00:02.462773\n",
      "3\n",
      "10000000\n",
      "0:00:03.307909\n",
      "4\n",
      "10000000\n",
      "0:00:02.428035\n",
      "5\n",
      "10000000\n",
      "0:00:02.227196\n"
     ]
    }
   ],
   "source": [
    "init = sc.parallelize(xrange(10000000), 3)\n",
    "init.cache()\n",
    "\n",
    "for i in range(6):\n",
    "    print i\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    init2 = init.map(lambda n: (n, n*3))        \n",
    "    init = init2.map(lambda n: n[0])    \n",
    "    init.cache()\n",
    "    \n",
    "    print init.count()    \n",
    "    print str(datetime.datetime.now() - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "time: 0:00:00.299933\n",
      "1\n",
      "10000\n",
      "time: 0:00:00.315004\n",
      "2\n",
      "10000\n",
      "time: 0:00:00.248155\n",
      "3\n",
      "10000\n",
      "time: 0:00:00.244719\n",
      "4\n",
      "10000\n",
      "time: 0:00:00.252504\n",
      "5\n",
      "10000\n",
      "time: 0:00:00.243843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arr = range(100)\n",
    "\n",
    "#     c = sc.broadcast(1)\n",
    "init = sc.parallelize(xrange(10000), 3)\n",
    "t= TimeHelper()\n",
    "\n",
    "for i in range(6):\n",
    "    print i\n",
    "\n",
    "    init2 = init.map(lambda n: n+1)\n",
    "    init3 = init2.flatMap(lambda n: [(n,1)])\n",
    "    \n",
    "    init4 = init3.reduceByKey(lambda x,y: x+y)\n",
    "    \n",
    "    \n",
    "#     print init5.take(3)\n",
    "    init = init4.map(lambda n: n[0])        \n",
    "#     print init4.take(3)\n",
    "    init.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    init.cache()\n",
    "    \n",
    "    \n",
    "    print init.count()\n",
    "    t.print_time_from_last()\n",
    "    \n",
    "    \n",
    "init.count()\n",
    "# t.print_time_from_last()\n",
    "# init.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
