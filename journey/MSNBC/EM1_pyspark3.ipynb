{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "from functools import reduce\n",
    "\n",
    "\"\"\"\n",
    "sources: \n",
    "http://windowoffice.tumblr.com/post/33548509/logsum-underflow-trick-re-discovery\n",
    "https://facwiki.cs.byu.edu/nlp/index.php/Log_Domain_Computations\n",
    "https://mikelove.wordpress.com/2011/06/06/log-probabilities-trick/\n",
    "\"\"\"\n",
    "def log_add(logx,logy):\n",
    "\n",
    "    if (logy > logx):\n",
    "        logy,logx = logx,logy\n",
    "        \n",
    "    negdiff = logy - logx\n",
    "\n",
    "    if negdiff < -20:\n",
    "        return logx\n",
    "\n",
    "    return logx + log(1.0 + exp(negdiff))\n",
    "\n",
    "def sum_logs(logs):\n",
    "    \"\"\"\n",
    "    return the log of total values\n",
    "    >>> exp(sum_logs([log(0.1),log(0.4),log(0.6)]))\n",
    "    1.1\n",
    "    \"\"\"\n",
    "    return reduce(log_add,logs) \n",
    "\n",
    "def normalize_logs(logs):\n",
    "    \"\"\"\n",
    "    convert the logs to normalized probablities\n",
    "    >>>normalize_logs(array([-1000,-1000,-990]))\n",
    "    array([  4.53958078e-05,   4.53958078e-05,   9.99909208e-01])\n",
    "    \"\"\"\n",
    "    log_total = sum_logs(logs)\n",
    "    logprobs = logs-log_total\n",
    "    probs = np.exp(logprobs)\n",
    "    return probs\n",
    "\n",
    "def pick_random_points(logprobs,num_points):\n",
    "    \"\"\"\n",
    "    picks random entries according to the logprobs vector\n",
    "    >>>np.random.seed(24)\n",
    "    >>>pick_random_points(array([log(0.1),log(0.1),log(0.5),log(0.3)]),num_points=10)\n",
    "    array([3, 2, 3, 2, 2, 3, 3, 2, 1, 2])\n",
    "    \"\"\"\n",
    "    probs = normalize_logs(logprobs)\n",
    "    xk = np.arange(len(probs))\n",
    "    custm = stats.rv_discrete(name='custm', values=(xk, probs))\n",
    "    return custm.rvs(size=num_points)\n",
    "\n",
    "def logprobs_to_normprobs(logprobs):\n",
    "    probs = np.exp(logprobs)\n",
    "    norm_prbs = probs/np.sum(probs)\n",
    "    return norm_prbs\n",
    "\n",
    "def logprobs_to_normprobs_safe(logprobs):\n",
    "#     this version uses log addition\n",
    "    logprob_total = sum_logs(logprobs)\n",
    "    norm_logprbs = logprobs-logprob_total\n",
    "    norm_prbs = np.exp(norm_logprbs)\n",
    "    return norm_prbs\n",
    "\n",
    "def calc_exp_log_prob(probs,logprobs):\n",
    "    return np.sum([0 if prob<=0 else prob*logprob for prob,logprob in zip(probs,logprobs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"app\").set(\"spark.executor.memory\", \"512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END_STATE = sc.broadcast(100)\n",
    "END_STATE_NAME = sc.broadcast(\"End\")\n",
    "START_STATE = sc.broadcast(0)\n",
    "STATE_NAMES = sc.broadcast(['Start','frontpage','news','tech','local','opinion','on-air','misc','weather','msn-news','health','living','business','msn-sports','sports','summary','bbs','travel'])\n",
    "num_partitions = 10\n",
    "# num_clusters = sc.broadcast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_traj(traj):\n",
    "    return [START_STATE.value]+list(traj)+[END_STATE.value,END_STATE.value]\n",
    "def unpad_traj(traj):\n",
    "    return traj[1:-2]\n",
    "def state_to_name(state):\n",
    "    if state==END_STATE:\n",
    "        return END_STATE_NAME\n",
    "    else:\n",
    "        return STATE_NAMES[state]\n",
    "def traj_to_namedstate(traj):\n",
    "    return map(state_to_name,traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EA_SMOOTH_TRANSITION = 0.1\n",
    "\n",
    "def create_smooth_transitions(states=[]):\n",
    "    return {(i,j):EA_SMOOTH_TRANSITION for i in states for j in states}\n",
    "\n",
    "def create_smooth_transitions_list(states=[]):\n",
    "    return [((i,j),EA_SMOOTH_TRANSITION) for i in states for j in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emit_ind_trans_prop(ind__traj__clsp_rob):\n",
    "    '''\n",
    "    >>> ind__traj__clsp_rob = (0, ([0, 1, 1, 100, 100], array([ 0.50891031,  0.26625345,  0.22483623]))))\n",
    "    >>> emit_ind_trans_prop(ind__traj__clsp_rob)\n",
    "    [((0, (0, 1)), 0.50891030999999998), ((0, (1, 1)), 0.50891030999999998), ((0, (1, 100)), 0.50891030999999998), ((0, (100, 100)), 0.50891030999999998), ((1, (0, 1)), 0.26625345), ((1, (1, 1)), 0.26625345), ((1, (1, 100)), 0.26625345), ((1, (100, 100)), 0.26625345), ((2, (0, 1)), 0.22483623), ((2, (1, 1)), 0.22483623), ((2, (1, 100)), 0.22483623), ((2, (100, 100)), 0.22483623)]\n",
    "    '''\n",
    "    index = ind__traj__clsp_rob[0]\n",
    "    traj = ind__traj__clsp_rob[1][0]\n",
    "    clusters_probs = ind__traj__clsp_rob[1][1]\n",
    "    \n",
    "    cl_step_propbs = []\n",
    "    \n",
    "    trans = zip(traj[:-1],traj[1:])\n",
    "    for c_ind, cls_p in enumerate(clusters_probs):\n",
    "        for step in trans:        \n",
    "            cl_step_propbs.append(((c_ind, step), (cls_p, index)))\n",
    "#             ini_p = smooth_trans_mtrx[step]\n",
    "#             cl_step_propbs.append(((c_ind, step), (ini_p + cls_p, index)))\n",
    "            #             yield ((c_ind, step), cls_p)\n",
    "        \n",
    "    return cl_step_propbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_list_based_on_another(base, dependent):\n",
    "    '''\n",
    "    >>> sort_list_based_on_another([0, 2, 1], [0.22, 0.55, 0.66])\n",
    "    ([0, 1, 2], [0.22, 0.66, 0.55])\n",
    "    '''\n",
    "    z = zip(base, dependent)\n",
    "    sz = sorted(z)\n",
    "    return [t[0] for t in sz], [t[1] for t in sz]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msnbc_no_header = sc.textFile(\"/ea/msnbc_no_header.seq\", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read trajectories\n",
    "list_traj = msnbc_no_header.map(lambda line: [int(i) for i in line.split()]) \\\n",
    "                           .filter(lambda seq: len(seq) <= 500) \\\n",
    "                           .map(pad_traj) \\\n",
    "                           .zipWithIndex() \\\n",
    "                           .map(lambda t: (t[1], t[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 1, 1, 100, 100]), (1, [0, 2, 100, 100])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_traj.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.33029398,  0.4153317 ,  0.24605769,  0.17777304,  0.53953642,\n",
       "        0.51564893,  0.91912959,  0.54545018,  0.01338867,  0.04689129]), array([ 0.00617019,  0.04893082,  0.12358357,  0.81829436,  0.32779294,\n",
       "        0.54885038,  0.79694105,  0.40638564,  0.74990192,  0.75861914]), array([ 0.34184011,  0.41096995,  0.96619596,  0.72706043,  0.60755683,\n",
       "        0.28279102,  0.50355959,  0.75314604,  0.49912015,  0.14816885]), array([ 0.0922865 ,  0.07525316,  0.83075079,  0.91919112,  0.15721844,\n",
       "        0.09215071,  0.82578609,  0.96152506,  0.7221524 ,  0.73515981])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_traj = 500\n",
    "list_traj_probs = sc.parallelize(xrange(num_traj), num_partitions) \\\n",
    "                                .map(lambda v: np.random.rand(1,num_clusters.value)[0]) \n",
    "    \n",
    "list_traj_probs.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def em_train2(list_traj, num_iterations=1):\n",
    "    \n",
    "    num_traj = list_traj.count()\n",
    "    \n",
    "    # measure_relative_state_size = states prior probs\n",
    "    states = list_traj.flatMap(lambda states: states[1])\n",
    "    states_counts = states.countByValue()\n",
    "    states_counts_sum = sum(states_counts.values())\n",
    "    states_dict = {k:float(v)/float(states_counts_sum)  for k,v in states_counts.iteritems()}\n",
    "#     states = sc.parallelize([k:float(v)/float(states_counts_sum)  for k,v in states_counts.iteritems()])\n",
    "#   [(step, EA_SMOOTH_TRANSITION), ]\n",
    "    smooth_mtrx_list = sc.parallelize(create_smooth_transitions_list(states_dict)).flatMap(lambda t:[((c,t[0]), t[1]) for c in range(num_clusters.value)])\n",
    "    \n",
    "    #init trajectories probs (init_traj_probs\n",
    "    # list_traj_probs = sc.parallelize(np.random.rand(num_traj,3),num_partitions).map(lambda vec: vec/np.sum(vec))\n",
    "    #sasha: probably need to make in list_traj, so no need to join\n",
    "    list_traj_probs = sc.parallelize(xrange(num_traj), num_partitions) \\\n",
    "                                .map(lambda v: np.random.rand(1,num_clusters.value)[0]) \\\n",
    "                                .map(lambda vec: vec/np.sum(vec)) \\\n",
    "                                .zipWithIndex() \\\n",
    "                                .map(lambda t: (t[1], t[0]))\n",
    "        \n",
    "    \n",
    "    \n",
    "#     global mikita_markov_models\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        print str(i) + ' iteration: ' + strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "        \n",
    "        # join trajectories and clusters initial random probs\n",
    "        # ((index), ([traj], [cluster_prob]))\n",
    "        list_traj__traj_probs = list_traj.join(list_traj_probs) \n",
    "        \n",
    "        # ((cluster, trans), (p, index))\n",
    "        # [((0, (0, 1)), (0.24919889991572008, 0)), ((0, (1, 1)), (0.24919889991572008, 0))]\n",
    "        list_cls_trans__p_index = list_traj__traj_probs.flatMap(emit_ind_trans_prop)\n",
    "        # ((cluster, trans), p)\n",
    "        # [((0, (0, 1)), 0.24919889991572008), ((0, (1, 1)), 0.24919889991572008)]\n",
    "        list_cls_trans__p = list_cls_trans__p_index.map(lambda t: (t[0],t[1][0]))\n",
    "        list_cls_trans__p.persist(StorageLevel.MEMORY_AND_DISK)        \n",
    "        # [((cluster, trans), p), ]  \n",
    "        # [((0, (0, 1)), 0.15297377614141211), ((0, (1, 1)), 0.15297377614141211)\n",
    "        list_cls_trans__p = list_cls_trans__p.union(smooth_mtrx_list)\n",
    "        \n",
    "        #################################  CALC MARKOV   ##############################\n",
    "        \n",
    "        # ((cluster, from), (trans, p))\n",
    "        # [((0, 0), ((0, 3), 0.45565240865632561)), ((0, 3), ((3, 100), 0.45565240865632561))]\n",
    "#         list_cls_from__trans_p = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1],t[1][0])))\n",
    "        list_cls_from__trans_p = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1],t[1])))\n",
    "        \n",
    "        # ((cluster, from), p)\n",
    "        # [((0, 0), 0.45565240865632561), ((0, 3), 0.45565240865632561)]\n",
    "        list_cls_from__p = list_cls_from__trans_p.map(lambda t: (t[0], t[1][1]))\n",
    "        # ((claster, from), summ)\n",
    "        # [((1, 3), 88045.381738379481), ((1, 13), 93509.65969107172), ((0, 100), 428820.63873856084), ((2, 10), 53400.141021192758)]\n",
    "        cls_from__summ = list_cls_from__p.reduceByKey(lambda x,y: x+y)\n",
    "    \n",
    "        # ((cluster,trans), sum)\n",
    "        # [((2, (16, 100)), 452.38469331913268), ((0, (15, 5)), 1081.6800474165561), ((0, (8, 8)), 142241.31289751496)]\n",
    "#         cls_trans__summ = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1]), t[1][0])) \\\n",
    "        cls_trans__summ = list_cls_trans__p.map(lambda t: ((t[0][0],t[0][1]), t[1])) \\\n",
    "                                                .reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "#         print cls_trans__summ.take(3)\n",
    "        \n",
    "        #  ((cluster, from), ((trans, trans_sum), from_summ))\n",
    "        # [((2, 12), (((12, 10), 1019.2083959882245), 113267.37822605985)), ((2, 12), (((12, 2), 3666.330047976217), 113267.37822605985))\n",
    "        cls_from__transsum_summ = cls_trans__summ.map(lambda t: ((t[0][0],t[0][1][0]), (t[0][1], t[1]))) \\\n",
    "                                                .join(cls_from__summ)\n",
    "\n",
    "#         print cls_from__transsum_summ.take(3)\n",
    "            \n",
    "        # [((2, (0, 4)), 0.05168807562712003), ((2, (0, 16)), 0.00042905633895048338), ((2, (0, 6)), 0.16451598030626441)]\n",
    "        markov_models = cls_from__transsum_summ.map(lambda t: ((t[0][0], t[1][0][0]), t[1][0][1]/t[1][1]))\n",
    "        \n",
    "#         markov_models_TF = cls_from__transsum_summ.map(lambda t: ((t[0][0], t[1][0][0]), t[1][0][1] > t[1][1]))\n",
    "        \n",
    "#         print markov_models_TF.filter(lambda t: t[1] == False).collect()\n",
    "        \n",
    "        #################################  CALC NEW PROBS   ##############################\n",
    "        \n",
    "        \n",
    "        # [((2, (16, 100)), ((0.42948178892760447, 264), 0.043625160028027611)), ((2, (16, 100)), ((0.52153298549282001, 262602), 0.043625160028027611))]\n",
    "        list_cls_trans__p_index_mrkp = list_cls_trans__p_index.join(markov_models)\n",
    "        list_cls_trans__p_index_mrkp.persist(StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        # ((cluster, index), (trans, logP))\n",
    "        # [((0, 789061), ((11, 7), -5.004076355127463)), ((0, 789061), ((11, 7), -5.004076355127463))]\n",
    "        list_cls_index__trans_logmrkp = list_cls_trans__p_index_mrkp.map(lambda t: ((t[0][0],t[1][0][1]), (t[0][1], math.log(t[1][1]))))\n",
    "\n",
    "        # walk_logprobs\n",
    "        # ((cluster, index), [(trans1, logP1),((trans2, logP2))])\n",
    "        # [ ((0, 784470), [((9, 100), -1.3163148315938333), ((0, 9), -2.717692054383914), ((100, 100), 0.0)])]\n",
    "        list_cls_index__grp_trans_logmrkp = list_cls_index__trans_logmrkp.groupByKey()\n",
    "        \n",
    "        # calc_walk_probs\n",
    "        # (index, (cluster, walk_logprob))\n",
    "        # [(784470, (0, -4.0340068859777469)), (950019, (1, -2.8974930003028816))\n",
    "        cls_index__sumlogmrkp = list_cls_index__grp_trans_logmrkp.map(lambda t: (t[0][1], (t[0][0], sum([tr_logp[1] for tr_logp in t[1]]))))\n",
    "        \n",
    "        # [(index, [(cluster1, walk_logprob1), (cluster2, walk_logprob2), (cluster3, walk_logprob3)], ]      \n",
    "        # [(0, [(0, -3.7289041683043873), (2, -3.7284874541593469), (1, -3.7306682864068987)], (655362, <pyspark.resultiterable.ResultIterable object at 0x7f01bf534790>)]\n",
    "        index__cls_sumlogmrkp = cls_index__sumlogmrkp.groupByKey()\n",
    "        \n",
    "        # cluster_probs = logprobs_to_normprobs_safe\n",
    "        # (index, [p1,p2,p3])\n",
    "        # [(655362, [0.3332682408091523, 0.3329057626978422, 0.333825996493006]), ]\n",
    "        list_traj_probs = index__cls_sumlogmrkp.map(lambda t: (t[0], \\\n",
    "                                    ([c_logp[0] for c_logp in t[1]], \\\n",
    "                                     logprobs_to_normprobs_safe(array([c_logp[1] for c_logp in t[1]]))))) \\\n",
    "                                  .map(lambda t: (t[0], (sort_list_based_on_another(t[1][0], t[1][1]))[1] ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        list_cls_trans__p.unpersist()\n",
    "        \n",
    "    list_assignment = list_traj_probs.map(lambda t: (t[0], np.argmax(t[1])))\n",
    "    return markov_models, list_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 07:09:17\n",
      "0 iteration: 2015-07-23 07:09:32\n",
      "1 iteration: 2015-07-23 07:09:32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-135-3e207a00a95b>\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    print mk.collect()\n",
      "  File \u001b[0;32m\"/usr/hdp/current/spark-client/python/pyspark/rdd.py\"\u001b[0m, line \u001b[0;32m713\u001b[0m, in \u001b[0;35mcollect\u001b[0m\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m536\u001b[0m, in \u001b[0;35m__call__\u001b[0m\n    answer = self.gateway_client.send_command(command)\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m364\u001b[0m, in \u001b[0;35msend_command\u001b[0m\n    response = connection.send_command(command)\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m473\u001b[0m, in \u001b[0;35msend_command\u001b[0m\n    answer = smart_decode(self.stream.readline()[:-1])\n",
      "\u001b[1;36m  File \u001b[1;32m\"/usr/local/lib/python2.7/socket.py\"\u001b[1;36m, line \u001b[1;32m430\u001b[1;36m, in \u001b[1;35mreadline\u001b[1;36m\u001b[0m\n\u001b[1;33m    data = recv(1)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "mk = em_train2(list_traj, 2)\n",
    "print mk.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_next_step(trans_vec):\n",
    "    next_step_vec = np.random.multinomial(1, trans_vec, size=1)[0]\n",
    "    return np.sum(next_step_vec*range(len(trans_vec)))\n",
    "\n",
    "def generate_test_trajs(trans_mtrx,num_trajs=100,traj_len=10):\n",
    "    trajs = []\n",
    "    for i in range(num_trajs):\n",
    "        test_traj = generate_test_traj(trans_mtrx,traj_len)\n",
    "        trajs.append(test_traj)\n",
    "    return trajs\n",
    "def generate_test_traj(trans_mtrx,traj_len=10):\n",
    "    cur_pos = 0\n",
    "    test_traj = [cur_pos]\n",
    "    for j in range(traj_len):\n",
    "        next_pos = calc_next_step(trans_mtrx[cur_pos])\n",
    "        test_traj.append(next_pos)\n",
    "        cur_pos = next_pos\n",
    "    return array(test_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans_mtrx = [[0.1,0.8,0.1],[0.4,0.2,0.4],[0.1,0.2,0.7]]\n",
    "trans_mtrx = [[0.8,0.1,0.1],[1,0,0],[0.9,0.1,0]]\n",
    "def generate_tarjs_per_cluster(list_trans_mtrx,num_trajs=100,traj_len=20):\n",
    "    trajs = []\n",
    "    labels = []\n",
    "    for index,trans_mtrx in enumerate(list_trans_mtrx):\n",
    "        cur_test_trajs = generate_test_trajs(trans_mtrx,num_trajs,traj_len)\n",
    "        trajs += cur_test_trajs\n",
    "        labels += [index]*num_trajs\n",
    "    \n",
    "    return DataFrame(data={'label':labels,'traj':trajs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_mtrx0 = [[0.8,0.1,0.05,0.05],[0.95,0,0,0.05],[0.9,0.05,0,0.05],[0.9,0.05,0,0.05]]\n",
    "trans_mtrx1 = [[0.1,0.8,0.05,0.05],[0,0.7,0.25,0.05],[0,0.75,0.1,0.15],[0,0.7,0.25,0.05]]\n",
    "trans_mtrx2 = [[0.1,0.15,0.7,0.05],[0.05,0.1,0.8,0.05],[0.2,0.15,0.6,0.05],[0.1,0.15,0.7,0.05]]\n",
    "list_trans_mtrx = [trans_mtrx0,trans_mtrx1,trans_mtrx2]\n",
    "num_trajs=100\n",
    "df_traj = generate_tarjs_per_cluster(list_trans_mtrx,num_trajs=num_trajs,traj_len=40)\n",
    "df_traj = df_traj.iloc[np.random.permutation(len(df_traj))]\n",
    "df_traj = df_traj.reset_index(drop=True)\n",
    "df_traj['is_train']=True\n",
    "df_traj.ix[df_traj.index<int(len(df_traj)*0.2),'is_train']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readBroadcastFromFile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:102)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1044)\n\tat org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:648)\n\tat org.apache.spark.api.python.PythonRDD$.readBroadcastFromFile(PythonRDD.scala:399)\n\tat org.apache.spark.api.python.PythonRDD.readBroadcastFromFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c528ad72f6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_trajs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_traj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_trajs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_traj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# train_walks = [traj_to_walk(traj) for traj in train_trajs]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark-client/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mbroadcast\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \"\"\"\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark-client/python/pyspark/broadcast.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sc, value, pickle_registry, path)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadBroadcastFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickle_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_registry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readBroadcastFromFile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:102)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1044)\n\tat org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:648)\n\tat org.apache.spark.api.python.PythonRDD$.readBroadcastFromFile(PythonRDD.scala:399)\n\tat org.apache.spark.api.python.PythonRDD.readBroadcastFromFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "num_clusters=sc.broadcast(3)\n",
    "train_trajs = df_traj[df_traj.is_train==True].traj\n",
    "test_trajs = df_traj[df_traj.is_train==True].traj\n",
    "sc.\n",
    "# train_walks = [traj_to_walk(traj) for traj in train_trajs]\n",
    "# test_walks = [traj_to_walk(traj) for traj in test_trajs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = [0,1,2,3]\n",
    "train_walks = trajs_to_walks(train_trajs,states)\n",
    "test_walks = trajs_to_walks(test_trajs,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_trajsRDD = sc.parallelize(train_trajs.tolist(),num_partitions) \\\n",
    "                        .map(lambda t: t.tolist()) \\\n",
    "                        .zipWithIndex() \\\n",
    "                        .map(lambda t: (t[1], t[0]))\n",
    "                        \n",
    "    \n",
    "# train_trajsRDD.take(3)\n",
    "# train_trajsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: 2015-07-23 09:06:35\n",
      "1 iteration: 2015-07-23 09:06:35\n",
      "2 iteration: 2015-07-23 09:06:35\n",
      "3 iteration: 2015-07-23 09:06:35\n",
      "4 iteration: 2015-07-23 09:06:35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-232-ff59f87e8037>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    assignments = assignmentsRDD.collect()\n",
      "  File \u001b[0;32m\"/usr/hdp/current/spark-client/python/pyspark/rdd.py\"\u001b[0m, line \u001b[0;32m713\u001b[0m, in \u001b[0;35mcollect\u001b[0m\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \u001b[0;32m\"/usr/hdp/current/spark-client/python/pyspark/traceback_utils.py\"\u001b[0m, line \u001b[0;32m78\u001b[0m, in \u001b[0;35m__exit__\u001b[0m\n    self._context._jsc.setCallSite(None)\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m536\u001b[0m, in \u001b[0;35m__call__\u001b[0m\n    answer = self.gateway_client.send_command(command)\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m364\u001b[0m, in \u001b[0;35msend_command\u001b[0m\n    response = connection.send_command(command)\n",
      "  File \u001b[0;32m\"/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py\"\u001b[0m, line \u001b[0;32m473\u001b[0m, in \u001b[0;35msend_command\u001b[0m\n    answer = smart_decode(self.stream.readline()[:-1])\n",
      "\u001b[1;36m  File \u001b[1;32m\"/usr/local/lib/python2.7/socket.py\"\u001b[1;36m, line \u001b[1;32m430\u001b[1;36m, in \u001b[1;35mreadline\u001b[1;36m\u001b[0m\n\u001b[1;33m    data = recv(1)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "markovsRDD, assignmentsRDD = em_train2(train_trajsRDD,5)\n",
    "assignments = assignmentsRDD.collect()\n",
    "markovs_by_cls = markovsRDD.map(lambda t: (t[0][0],(t[0][1],t[1]))).groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markovs_dict = {}\n",
    "for c in markovs_by_cls:    \n",
    "    m = {tr_p[0]:tr_p[1] for tr_p in c[1]}\n",
    "    markovs_dict[c[0]] = m\n",
    "markovs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assignments.sort(key=lambda t: t[0])\n",
    "list_assignment = [ass[1] for ass in assigments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_traj[df_traj.is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_assign = range(3)\n",
    "df = DataFrame({'real':array(df_traj[df_traj.is_train==True].label),'assign':list_assignment})\n",
    "real_assign = range(3)\n",
    "df['compare'] = df.apply(lambda x: real_assign[x['real']]==x['assign'],axis=1)\n",
    "len(df[df.compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1, 2), 93)\n"
     ]
    }
   ],
   "source": [
    "# all_combs = itertools.chain(*(itertools.combinations(filter_vars, i) for i in range(1,3)))\n",
    "import itertools\n",
    "assigns = {}\n",
    "for real_assign in itertools.permutations(range(3)):\n",
    "    df['compare'] = df.apply(lambda x: real_assign[x['real']]==x['assign'],axis=1)\n",
    "    num_eq = len(df[df.compare])\n",
    "    assigns[real_assign] = num_eq\n",
    "\n",
    "import operator\n",
    "print max(assigns.iteritems(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# arr = range(100)\n",
    "\n",
    "\n",
    "init = sc.parallelize(xrange(10000), 3)\n",
    "\n",
    "for i in range(5):\n",
    "    print i\n",
    "    c = sc.broadcast(1)\n",
    "    init = init.map(lambda n: n+c.value)\n",
    "    \n",
    "# init.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 10), (2, 20), (3, 30)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(100), 3)\n",
    "rdd1 = rdd.map(lambda t: ({'from':t, 'to':t+1}, {t}))\n",
    "rdd2 = rdd1.map(lambda t:  (t[0]['from'], {t[1]*10}))\n",
    "rdd2.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_jrdd_deserializer'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-65-c69f16fbe4e7>\"\u001b[0m, line \u001b[0;32m5\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    rdd3 = rdd2.union([({'from':222}, 9999)])\n",
      "\u001b[1;36m  File \u001b[1;32m\"/usr/hdp/current/spark-client/python/pyspark/rdd.py\"\u001b[1;36m, line \u001b[1;32m475\u001b[1;36m, in \u001b[1;35munion\u001b[1;36m\u001b[0m\n\u001b[1;33m    if self._jrdd_deserializer == other._jrdd_deserializer:\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m 'list' object has no attribute '_jrdd_deserializer'\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(10), 3)\n",
    "rdd1 = rdd.map(lambda t: ({'from':t, 'to':t+1}, {t}))\n",
    "\n",
    "rdd2 = rdd1.map(lambda t: (t[0]['from'], {t[1]*10}))\n",
    "rdd3 = rdd2.union([({'from':222}, 9999)])\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "print sys.getsizeof((1,2))\n",
    "print sys.getsizeof({'from':1, 'to':2})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
