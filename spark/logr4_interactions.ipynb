{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import sql\n",
    "from pyspark import SparkConf\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import GroupedData\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation  import *\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "from datasu.auc import *\n",
    "from datasu.dicts import *\n",
    "from datasu.files import *\n",
    "from datasu.pandas import *\n",
    "from datasu.persist import *\n",
    "from datasu.spark import *\n",
    "from datasu.patsy import *\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.name', u'logr3'),\n",
       " (u'spark.master', u'spark://spark1.ea.lab:7077'),\n",
       " (u'spark.executor.max', u'3'),\n",
       " (u'spark.driver.memory', u'12g'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.jars',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.executor.memory', u'5g'),\n",
       " (u'spark.driver.maxResultSize', u'5g'),\n",
       " (u'spark.files',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.cores.max', u'28'),\n",
       " (u'spark.worker.cleanup.enabled', u'True'),\n",
       " (u'spark.python.worker.memory', u'2g'),\n",
       " (u'spark.executor.extraJavaOptions',\n",
       "  u'-XX:+PrintGCDetails -XX:+UseCompressedOops'),\n",
       " (u'spark.submit.deployMode', u'client')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '12g')\n",
    "conf.set('spark.python.worker.memory', '2g')\n",
    "conf.set(\"spark.driver.maxResultSize\", \"5g\")\n",
    "conf.set(\"spark.executor.max\", 3)\n",
    "conf.set('spark.executor.memory', '5g')\n",
    "conf.set(\"spark.cores.max\", 28)\n",
    "conf.set('spark.worker.cleanup.enabled', True)\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set('spark.executor.extraJavaOptions', '-XX:+PrintGCDetails -XX:+UseCompressedOops')\n",
    "\n",
    "conf.setAppName('logr3')\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print 'spark context not exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqc = pyspark.SQLContext(sc)\n",
    "sqlContext = sqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = sqc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_data_path = '/home/ds/dev/data/Kagle-ValuesShoppers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_data_path = 'file://'+ base_data_path + 'spark_data/'\n",
    "transactions_name = 'transactions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = csv_reader.load(spark_data_path+'ddf_trans_agg_history', samplingRatio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_trans_agg_history.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105104330"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_trans_agg_history.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = ddf_trans_agg_history.withColumnRenamed('repeater_calc', 'repeater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'category',\n",
       " 'brand',\n",
       " 'agg_customer_id_brand_customer_id_count',\n",
       " 'agg_customer_id_brand_productsize_average',\n",
       " 'agg_customer_id_brand_productsize_total',\n",
       " 'agg_customer_id_brand_purchasequantity_average',\n",
       " 'agg_customer_id_brand_purchasequantity_total',\n",
       " 'agg_customer_id_brand_purchaseamount_average',\n",
       " 'agg_customer_id_brand_purchaseamount_total',\n",
       " 'agg_customer_id_category_customer_id_count',\n",
       " 'agg_customer_id_category_productsize_average',\n",
       " 'agg_customer_id_category_productsize_total',\n",
       " 'agg_customer_id_category_purchasequantity_average',\n",
       " 'agg_customer_id_category_purchasequantity_total',\n",
       " 'agg_customer_id_category_purchaseamount_average',\n",
       " 'agg_customer_id_category_purchaseamount_total',\n",
       " 'repeater']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_trans_agg_history.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_coupons = pd.read_csv(base_data_path+'offers')[['offer','category','company','brand','offervalue','quantity']]\n",
    "df_offers_ids = pd.read_csv(base_data_path+'trainHistory').rename(columns={'id': 'customer_id'})\n",
    "df_offers_ids_submission = pd.read_csv(base_data_path+'testHistory').rename(columns={'id': 'customer_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_offers_all = pd.merge(df_offers_ids, df_coupons, on=['offer'])\n",
    "df_offers_all = df_offers_all[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity','repeattrips','repeater']]\n",
    "\n",
    "df_offers_all_submission = pd.merge(df_offers_ids_submission, df_coupons, on=['offer'])\n",
    "df_offers_all_submission = df_offers_all_submission[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['df_offers_submission__trans_aggs.var', 'df_offers__trans_aggs.var']\n",
      "loaded df_offers_submission__trans_aggs\n",
      "loaded df_offers__trans_aggs\n"
     ]
    }
   ],
   "source": [
    "main_folder = '/home/ds/dev/data/Kagle-ValuesShoppers/'\n",
    "load_variables(path=main_folder+'working_data', variables=['df_offers__trans_aggs', 'df_offers_submission__trans_aggs']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers__trans_aggs = sqc.createDataFrame(df_offers__trans_aggs).na.fill(0)\n",
    "ddf_offers_submission__trans_aggs = sqc.createDataFrame(df_offers_submission__trans_aggs).na.fill(0)\n",
    "# ddf_offers__trans_aggs = ddf_offers__trans_aggs.drop('repeattrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['chain','market','category','company','brand']\n",
    "agg_cols = filter(lambda c: re.match(\"agg_*\",c), ddf_offers__trans_aggs.columns)\n",
    "num_cols = ['offervalue','quantity']\n",
    "\n",
    "ddf_offers__trans_aggs = convert_columns_to_type(ddf_offers__trans_aggs, categorical_cols, StringType)\n",
    "ddf_offers_submission__trans_aggs = convert_columns_to_type(ddf_offers_submission__trans_aggs, categorical_cols, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = convert_columns_to_type(ddf_trans_agg_history, categorical_cols, StringType)\n",
    "ddf_trans_agg_history = ddf_trans_agg_history.withColumn('repeater', F.substring(ddf_trans_agg_history.repeater, 0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plus_categorical_expr = partial(plus_expr, expr='%s')\n",
    "# categorical_cols_plus = plus_categorical_expr(categorical_cols)\n",
    "\n",
    "# brand_interactions = \"(%s):(%s)\" % (plus_agg_columns_by_infix('brand',agg_cols), categorical_cols_plus)\n",
    "# category_interactions = \"(%s):(%s)\" % (plus_agg_columns_by_infix('category',agg_cols), categorical_cols_plus)\n",
    "# num_interactions = \"(%s):(%s)\" % (plus_expr(agg_cols), plus_expr(num_cols))\n",
    "\n",
    "# R_expr = plus_expr([brand_interactions,category_interactions,num_interactions])\n",
    "# R_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build prepare pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf1 = RFormula(formula=\"repeater ~ . - repeattrips\", featuresCol=\"features\", labelCol=\"label\")\n",
    "# rf1 = RFormula(formula=\"repeater ~ agg_customer_id_brand_customer_id_count\", featuresCol=\"features\", labelCol=\"label\")\n",
    "rf1 = RFormula(formula=\"repeater ~ category:. + brand:. -repeattrips -chain -offer -market -company -offerdate -offervalue -quantity\", featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "\n",
    "\n",
    "pipe_transform1 = Pipeline(stages=[rf1])\n",
    "\n",
    "std_scale = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=False)\n",
    "\n",
    "px = PolynomialExpansion(degree=2, inputCol=\"features_scaled\", outputCol=\"features_poly\")\n",
    "\n",
    "# pipe_prepare1 = Pipeline(stages=[std_scale, px])\n",
    "pipe_prepare1 = Pipeline(stages=[std_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"label\", predictionCol=\"prediction\", regParam=0.1, elasticNetParam=0.3)\n",
    "\n",
    "# , \n",
    "#                          maxIter=100, regParam=0.1, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \n",
    "#                          thresholds=None, probabilityCol=\"probability\",  \n",
    "#                          rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_lr1 = Pipeline(stages=[pipe_prepare1, lr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers_submission__trans_aggs = ddf_offers_submission__trans_aggs.withColumn('repeater',F.lit('f'))\n",
    "ddf_offers_union__trans_aggs = ddf_offers__trans_aggs.unionAll(\n",
    "                            ddf_offers_submission__trans_aggs.select(*ddf_offers__trans_aggs.columns))\n",
    "\n",
    "ddf_offers_union__trans_aggs = ddf_offers_union__trans_aggs.drop('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_transform2 = pipe_transform1.fit(ddf_offers_union__trans_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data = model_transform2.transform(ddf_offers__trans_aggs)\n",
    "ddf_data = ddf_data.select(['features','label']) #, 'repeattrips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_train, ddf_data_test = ddf_data.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(14369, {13: 1.0, 280: 1.0, 2154: 1.0, 3092: 1204821.0, 3549: 1.0, 4026: 1.0, 6081: 1.0, 7052: 1.5, 7072: 1.0, 7373: 1.0, 7670: 1.0, 9646: 1.0, 10317: 1204821.0, 10806: 1.0, 11233: 1.0, 13397: 1.0, 14079: 1.5, 14098: 1.0}), label=0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_data_train.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ddf_data_train.groupBy('label').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr2 = pipe_lr1.fit(ddf_data_train) #, params={'weightCol':\"repeattrips\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "ddf_data_test_res1 = model_lr2.transform(ddf_data_test, params={'threshold':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features',\n",
       " 'label',\n",
       " 'features_scaled',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_data_test_res1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|label|     features_scaled|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(14369,[1,39,575,...|  0.0|(14369,[1,39,575,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,552,...|  0.0|(14369,[1,39,552,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[5,118,111...|  1.0|(14369,[5,118,111...|[1.07549227353990...|[0.74563999050520...|       0.0|\n",
      "|(14369,[5,118,108...|  0.0|(14369,[5,118,108...|[1.07549227353990...|[0.74563999050520...|       0.0|\n",
      "|(14369,[1,39,572,...|  0.0|(14369,[1,39,572,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,546,...|  0.0|(14369,[1,39,546,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,549,...|  0.0|(14369,[1,39,549,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,575,...|  0.0|(14369,[1,39,575,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,546,...|  0.0|(14369,[1,39,546,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "|(14369,[1,39,549,...|  1.0|(14369,[1,39,549,...|[1.11769824504102...|[0.75356151497684...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf_data_test_res1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-16c4a4569901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_data_test_res1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddf_data_test_res1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rawPrediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_index_from_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ds/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \"\"\"\n\u001b[0;32m   1377\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m     \u001b[1;31m##########################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/home/ds/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_data_test_res1 = ddf_data_test_res1.select(['rawPrediction', get_index_from_vector()('probability', F.lit(1)).alias('probability'), 'prediction', 'label']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_test_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_auc(df_data_test_res1.label, df_data_test_res1.probability.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder().build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=pipe_lr1, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "cvModel = cv.fit(ddf_data_train)\n",
    "evaluator.evaluate(cvModel.transform(ddf_data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit big data!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = ddf_trans_agg_history.drop('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history_union_offers = ddf_trans_agg_history.unionAll(ddf_offers_union__trans_aggs.select(*ddf_trans_agg_history.columns))\n",
    "ddf_trans_agg_history_union_offers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_transform1 = pipe_transform1.fit(ddf_trans_agg_history_union_offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data = model_transform1.transform(ddf_trans_agg_history)\n",
    "ddf_data = ddf_data.select(['features','label']) #, 'repeattrips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_train = ddf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test = model_transform1.transform(ddf_offers__trans_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ddf_data_train, ddf_data_test = ddf_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history_union_offers.groupBy('repeater').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr1 = pipe_lr1.fit(ddf_data_train) #, params={'weightCol':\"repeattrips\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1 = model_lr1.transform(ddf_data_test, params={'threshold':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1 = ddf_data_test_res1.select(['rawPrediction', get_index_from_vector()('probability', F.lit(1)).alias('probability'), 'prediction', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_data_test_res1 = ddf_data_test_res1.sample(False, fraction=0.1).toPandas()\n",
    "df_data_test_res1 = ddf_data_test_res1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_test_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_auc(df_data_test_res1.label, df_data_test_res1.probability.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_submission = model_transform2.transform(ddf_offers_submission__trans_aggs).select(['customer_id','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_submission_res1 = model_lr2.transform(ddf_data_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1 = ddf_data_submission_res1.select(\n",
    "                [F.col('customer_id').alias('id'), get_index_from_vector()('probability', F.lit(1)).alias('repeatProbability')]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1.to_csv(path_or_buf=main_folder+'submission/'+'submission_spark18_LR1_bd', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "output = ParamGridBuilder() \\\n",
    "         .baseOn({lr.labelCol: 'l'}) \\\n",
    "         .baseOn([lr.predictionCol, 'p']) \\\n",
    "         .addGrid(lr.regParam, [1.0, 2.0]) \\\n",
    "         .addGrid(lr.maxIter, [1, 5]) \\\n",
    "         .build()\n",
    "            \n",
    "expected = [\n",
    "         {lr.regParam: 1.0, lr.maxIter: 1, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 2.0, lr.maxIter: 1, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 1.0, lr.maxIter: 5, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 2.0, lr.maxIter: 5, lr.labelCol: 'l', lr.predictionCol: 'p'}]\n",
    "\n",
    "len(output) == len(expected)\n",
    "all([m in expected for m in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "dataset = sqlContext.createDataFrame(\n",
    "     [(Vectors.dense([0.0]), 0.0),\n",
    "      (Vectors.dense([0.4]), 1.0),\n",
    "      (Vectors.dense([0.5]), 0.0),\n",
    "      (Vectors.dense([0.6]), 1.0),\n",
    "      (Vectors.dense([1.0]), 1.0)] * 10,\n",
    "     [\"features\", \"label\"])\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(dataset)\n",
    "evaluator.evaluate(cvModel.transform(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play with RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqc.createDataFrame([\n",
    "     (1.0, 1.0, \"a\",\"q\"),\n",
    "     (0.0, 2.0, \"b\",\"w\"),\n",
    "     (0.0, 3.0, \"a\",\"q\"),    \n",
    " ], [\"y\", \"x\", \"cat1\",\"cat2\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RFormula(formula=\"y ~ cat1\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df1 = rf.fit(df).transform(df)\n",
    "df1.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1+cat2\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df2 = rf.fit(df).transform(df)\n",
    "df2.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1:cat2\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df3 = rf.fit(df).transform(df)\n",
    "df3.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1:x + cat2:x\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df4 = rf.fit(df).transform(df)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(df1)\n",
    "scalerModel.transform(df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "df = sc.parallelize([\n",
    "     Row(label=1.0, weight=2.0, features=Vectors.dense(1.0)),\n",
    "     Row(label=0.0, weight=2.0, features=Vectors.sparse(1, [], []))]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense(-1.0))]).toDF()\n",
    "result = model.transform(test0).head()\n",
    "result.prediction\n",
    "\n",
    "result.probability\n",
    "\n",
    "result.rawPrediction\n",
    "\n",
    "test1 = sc.parallelize([Row(features=Vectors.sparse(1, [0], [1.0]))]).toDF()\n",
    "model.transform(test1).head().prediction\n",
    "\n",
    "lr.setParams(\"vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
