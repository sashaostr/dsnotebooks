{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import sql\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import GroupedData\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql import GroupedData\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from datasu.auc import *\n",
    "\n",
    "from datasu.dicts import *\n",
    "from datasu.files import *\n",
    "from datasu.pandas import *\n",
    "from datasu.persist import *\n",
    "from datasu.spark import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.master', u'spark://spark1.ea.lab:7077'),\n",
       " (u'spark.executor.max', u'3'),\n",
       " (u'spark.driver.memory', u'12g'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.jars',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.executor.memory', u'5g'),\n",
       " (u'spark.app.name', u'prepare features'),\n",
       " (u'spark.driver.maxResultSize', u'5g'),\n",
       " (u'spark.files',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.cores.max', u'28'),\n",
       " (u'spark.worker.cleanup.enabled', u'True'),\n",
       " (u'spark.python.worker.memory', u'2g'),\n",
       " (u'spark.executor.extraJavaOptions',\n",
       "  u'-XX:+PrintGCDetails -XX:+UseCompressedOops'),\n",
       " (u'spark.submit.deployMode', u'client')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '12g')\n",
    "conf.set('spark.python.worker.memory', '2g')\n",
    "conf.set(\"spark.driver.maxResultSize\", \"5g\")\n",
    "conf.set(\"spark.executor.max\", 3)\n",
    "conf.set('spark.executor.memory', '5g')\n",
    "conf.set(\"spark.cores.max\", 28)\n",
    "conf.set('spark.worker.cleanup.enabled', True)\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set('spark.executor.extraJavaOptions', '-XX:+PrintGCDetails -XX:+UseCompressedOops')\n",
    "\n",
    "conf.setAppName('prepare features')\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print 'spark context not exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqc = pyspark.SQLContext(sc)\n",
    "# shq = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism, sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_reader = sqc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_data_path = '/home/ds/dev/data/Kagle-ValuesShoppers/'\n",
    "spark_data_path = 'file://'+ base_data_path + 'spark_data/'\n",
    "transactions_name = 'transactions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_coupons = pd.read_csv(base_data_path+'offers')[['offer','category','company','brand','offervalue','quantity']]\n",
    "df_offers_ids = pd.read_csv(base_data_path+'trainHistory').rename(columns={'id': 'customer_id'})\n",
    "df_offers_ids_subm = pd.read_csv(base_data_path+'testHistory').rename(columns={'id': 'customer_id'})\n",
    "# df_trans_all = pd.read_csv(base_data_path+'transactions_reduced_category').rename(columns={'id': 'customer_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_offers_hist = pd.merge(df_offers_ids, df_coupons, on=['offer'])\n",
    "df_offers_hist = df_offers_hist[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity','repeattrips','repeater']]\n",
    "\n",
    "df_offers_subm = pd.merge(df_offers_ids_subm, df_coupons, on=['offer'])\n",
    "df_offers_subm = df_offers_subm[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions = csv_reader.load(base_data_path+transactions_name, samplingRatio=0.02)\n",
    "ddf_transactions.rdd.setName(transactions_name)\n",
    "ddf_transactions.alias('transactions')\n",
    "\n",
    "ddf_transactions.rdd.getNumPartitions()\n",
    "ddf_transactions = ddf_transactions.withColumnRenamed('id','customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions_small, ddf_transactions_big = ddf_transactions.randomSplit([0.05,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: bigint, chain: int, dept: int, category: int, company: bigint, brand: int, date: string, productsize: double, productmeasure: string, purchasequantity: int, purchaseamount: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_trans = ddf_transactions_small\n",
    "ddf_trans.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17484788"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_trans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['chain','market','category','company','brand']\n",
    "num_cols = ['offervalue','quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.select('dept').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.select('category').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.select('company').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.select('brand').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions.select('customer_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers_hist = sqc.createDataFrame(df_offers_hist)\n",
    "ddf_offers_subm = sqc.createDataFrame(df_offers_subm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_offers_hist_ids = ddf_offers_hist.select('customer_id')\n",
    "ddf_offers_all_ids = ddf_offers_hist_ids.unionAll(ddf_offers_subm.select('customer_id')).distinct()\n",
    "\n",
    "ddf_transactions = ddf_transactions.join(ddf_offers_all_ids, on='customer_id', how='leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_transactions = ddf_transactions.repartition(12000)\n",
    "ddf_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PREPARE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ_grouping = {'total':F.sum, 'average':F.avg }\n",
    "count_grouping = {'count':F.count }\n",
    "\n",
    "# count_agg = partial(get_ddf_aggs, agg_columns=['customer_id'], agg_funcs=count_grouping, prefix='agg_')\n",
    "# total_avg_agg = partial(get_ddf_aggs, agg_columns=['productsize','purchasequantity','purchaseamount'], agg_funcs=summ_grouping, prefix='agg_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grpby_columns = ['customer_id','brand', 'category', 'dept']\n",
    "grpby_columns = ['customer_id','category']\n",
    "grpby_columns_name = ['customer_id','brand']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## effective pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as sql_types\n",
    "# import pyspark.sql.\n",
    "\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "from pyspark.mllib.linalg import VectorUDT, MatrixUDT, Vectors, Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ddf_aggs(grpby_columns, agg_columns, agg_funcs, prefix=None, suffix=None, cast_to=None, return_columns_names=False):\n",
    "    \"\"\"\n",
    "    generates aggregations for spark dataframe\n",
    "    :param grpby_columns: columns to groupby with: ['id','brand']\n",
    "    :param agg_columns: columns to aggregate: ['productsize','purchasequantity']\n",
    "    :param agg_funcs: aggregations dict to enable on agg_columns: { 'total':F.sum, 'average':F.avg }\n",
    "    :param cast_to: cast aggregation result column to type (e.g. cast_to='double')\n",
    "    :return [Column<avg(productsize) AS id_brand_productsize_average#59>,\n",
    "             Column<sum(productsize) AS id_brand_productsize_total#60>,\n",
    "             Column<avg(purchasequantity) AS id_brand_purchasequantity_average#61>,\n",
    "             Column<sum(purchasequantity) AS id_brand_purchasequantity_total#62>]:\n",
    "\n",
    "    Example:\n",
    "\n",
    "    total_avg_agg = partial(get_ddf_aggs, agg_columns=['productsize','purchasequantity',],\n",
    "                                     agg_funcs={'total':np.sum, 'average':np.average })\n",
    "\n",
    "    grpby_columns = ['customer_id','brand']\n",
    "\n",
    "    df_trans_grp_customer_brand = dff_trans.groupby(grpby_columns)\n",
    "                                          .agg(**total_avg_agg)\n",
    "    \"\"\"\n",
    "    aggs = []\n",
    "    col_names = []\n",
    "    col_prefix = prefix + '_'.join(grpby_columns)\n",
    "    for col in agg_columns:\n",
    "        for agg_name, agg_func in agg_funcs.iteritems():\n",
    "            agg_f = agg_func(col)\n",
    "            if cast_to:\n",
    "                agg_f = agg_f.cast(cast_to)\n",
    "            alias = \"_\".join([s for s in [col_prefix, col, agg_name, suffix] if s])    \n",
    "            agg = agg_f.alias(alias)\n",
    "            aggs.append(agg)\n",
    "            col_names.append(alias)\n",
    "            \n",
    "    if return_columns_names:\n",
    "        return aggs, col_names\n",
    "    else:\n",
    "        return aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_avg_agg = partial(get_ddf_aggs, agg_columns=['productsize'], agg_funcs={'total':F.sum}, prefix='agg_', cast_to='double', return_columns_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['customer_id', 'category']\n",
    "agg, agg_column = total_avg_agg(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg, agg_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated = ddf_trans.groupBy(*map(lambda c: F.col(c),cols)).agg(*agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = map(lambda c: StringIndexer(inputCol=c, outputCol='%s_idx' % c).fit(aggregated), ['category']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated = reduce(lambda ddf,t: t.transform(ddf), indexers, aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def columns_to_tuple(*values):\n",
    "    def to_tuple(*values):\n",
    "        return str(tuple(values))\n",
    "    \n",
    "    return UserDefinedFunction(to_tuple, StringType(), 'columns_to_tuple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated2 = aggregated.withColumn('tuple', columns_to_tuple()(aggregated.category_idx, aggregated.agg_customer_id_category_productsize_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated2.select('tuple').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregated2_gr_cust = aggregated2.groupBy('customer_id_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated2_gr_cust_agg = aggregated2_gr_cust.agg(F.collect_list(F.col('tuple')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated2_gr_cust_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_str(*values):\n",
    "    \n",
    "    return UserDefinedFunction(lambda arr: ','.join(arr), StringType(), 'join_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregated2_gr_cust_agg = aggregated2_gr_cust_agg.withColumn('tuples',join_str()(aggregated2_gr_cust_agg['collect_list(tuple)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated2_gr_cust_agg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vectors.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idea: use Vectors.parse(' ( 100,  [0],  [2])')\n",
    "# 1. select max category_idx\n",
    "# 2. collect_list for category_idx, agg_customer_id_category_productsize_total\n",
    "# 3. join_str for each of results, so get two arrays as string in two separete columns\n",
    "# 4. use F.concat_ws(',',F.lit(max index), string of index array, string of values array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_cols = map(lambda c: F.col('%s_idx'%c).cast('long'),cols) + [F.col('agg_customer_id_category_productsize_total').cast('double')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed = aggregated.select(pivot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed.select('category_idx').rdd.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed2 = indexed.withColumn('vector', F.concat_ws(':',indexed_gr_cust_agg['customer_id_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_gr_cust = indexed.groupBy('customer_id_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_gr_cust_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vectors.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_sparse(indices, values):\n",
    "    return Vectors.sparse(4, [1, 3], [3.0, 4.0])\n",
    "\n",
    "\n",
    "collect_to_sparse_vector = UserDefinedFunction(to_sparse, sql_types.UserDefinedType, 'collect_to_sparse_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = F.col('agg_customer_id_category_productsize_total')\n",
    "inds = F.col('category_idx')\n",
    "\n",
    "# indexed_gr_cust_agg = indexed_gr_cust.agg(F.UserDefinedFunction(c1),F.collect_list(c2))\n",
    "indexed_gr_cust_agg = indexed_gr_cust.agg(collect_to_sparse_vector(inds, vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = indexed_gr_cust_agg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vectors.sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_convert_to_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = indexed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_columns('.*productsize.*' ,aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get():\n",
    "    return 1,2\n",
    "\n",
    "a = get()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.select(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(aggregated.columns)-set(['category', 'category_idx', 'customer_id']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, IndexedRowMatrix\n",
    "cm = CoordinateMatrix(\n",
    "    aggregated.map(lambda r: (r['customer_id'], r.category_idx, r.agg_customer_id_category_productsize_total))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irm = cm.toIndexedRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irm.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_irm = irm.rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_irm.withColumnRenamed('index', 'customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,1), (0,1), (0,2), (1,2)])\n",
    "sqc.createDataFrame(rdd, [\"id\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rm.rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# def aggregate(ddf, grpby_columns, aggs):\n",
    "#     aggregated = ddf.groupBy(*map(lambda c: F.col(c),grpby_columns)).agg(*aggs)\n",
    "#     return aggregated\n",
    "    \n",
    "def index_columns(ddf, index_columns, index_col_suffix='_idx'):     \n",
    "    indexers = map(lambda c: StringIndexer(inputCol=c, outputCol='%s%s' % (c,index_col_suffix)).fit(ddf), index_columns) \n",
    "    indexed = reduce(lambda ddf,t: t.transform(ddf), indexers, ddf)    \n",
    "    return indexed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate_pivot_to_sparse_vector(ddf, id_column, pivot_column, aggs, vector_column_name='features'):\n",
    "    from pyspark.mllib.linalg.distributed import CoordinateMatrix, IndexedRowMatrix\n",
    "\n",
    "    index_col_suffix = '_idx'\n",
    "    grpby_columns = [id_column, pivot_column]\n",
    "\n",
    "    aggregated = ddf.groupBy(grpby_columns).agg(*aggs)\n",
    "    \n",
    "    pivot_indexed_column = pivot_column+index_col_suffix\n",
    "    agg_column_names = list(set(aggregated.columns)-set([id_column, pivot_column, pivot_indexed_column]))\n",
    "\n",
    "    indexed = index_columns(ddf=aggregated, index_columns=[pivot_column])\n",
    "\n",
    "    res = None\n",
    "    agg_columns_vectors = map(lambda c: c+'_vector',agg_column_names)\n",
    "    for agg_column, agg_column_vector in zip(agg_column_names, agg_columns_vectors):\n",
    "        print agg_column, agg_column_vector\n",
    "        \n",
    "        cm = CoordinateMatrix(\n",
    "            indexed.map(lambda r: (long(r[id_column]), long(r[pivot_indexed_column]), r[agg_column]))\n",
    "        )\n",
    "        irm = cm.toIndexedRowMatrix()\n",
    "        ddf_irm = irm.rows.toDF()\n",
    "        ddf_irm = ddf_irm.withColumnRenamed('index', id_column).withColumnRenamed('vector', agg_column_vector)\n",
    "\n",
    "        if res:\n",
    "            res = res.join(ddf_irm, on=id_column, how='inner')\n",
    "        else:\n",
    "            res = ddf_irm\n",
    "\n",
    "    \n",
    "    if len(agg_columns_vectors)>1:\n",
    "        assembler = VectorAssembler(inputCols=agg_columns_vectors, outputCol=vector_column_name)\n",
    "        res = assembler.transform(res)\n",
    "    else:\n",
    "        res = res.withColumnRenamed(agg_columns_vectors[0], vector_column_name)\n",
    "    \n",
    "    res = drop_columns(res, columns=agg_columns_vectors)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_features(ddfs, join_column, merge_column, output_column='features', drop_merged_columns=True):       \n",
    "    ddf_res = ddfs.pop(0)\n",
    "    merge_column_renamed = merge_column + str(0)\n",
    "    merge_columns = [merge_column_renamed]\n",
    "    ddf_res = ddf_res.withColumnRenamed(merge_column, merge_column_renamed)\n",
    "    \n",
    "    for i,ddf in enumerate(ddfs):     \n",
    "        merge_column_renamed = merge_column + str(i+1)\n",
    "        merge_columns.append(merge_column_renamed)\n",
    "        ddf_r = ddf.withColumnRenamed(merge_column, merge_column_renamed)\n",
    "        ddf_res = ddf_res.join(ddf_r, on=join_column, how='inner')\n",
    "    \n",
    "    assembler = VectorAssembler( inputCols=merge_columns, outputCol=output_column)\n",
    "    res = assembler.transform(ddf_res)\n",
    "    \n",
    "    if drop_merged_columns:\n",
    "        res = drop_columns(res, columns=merge_columns)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_avg_agg = partial(get_ddf_aggs, agg_columns=['productsize','purchasequantity'], agg_funcs={'total':F.sum}, prefix='agg_', cast_to='double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<cast((sum(productsize),mode=Complete,isDistinct=false) as double) AS agg_customer_id_category_productsize_total#127>,\n",
       " Column<cast((sum(purchasequantity),mode=Complete,isDistinct=false) as double) AS agg_customer_id_category_purchasequantity_total#128>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['customer_id', 'category']\n",
    "aggs = total_avg_agg(cols)\n",
    "aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_customer_id_category_purchasequantity_total agg_customer_id_category_purchasequantity_total_vector\n",
      "agg_customer_id_category_productsize_total agg_customer_id_category_productsize_total_vector\n"
     ]
    }
   ],
   "source": [
    "ddf_pivot1 = aggregate_pivot_to_sparse_vector(ddf_trans, id_column='customer_id', \n",
    "                                              pivot_column='category', \n",
    "                                              aggs=total_avg_agg(['customer_id', 'category']))\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(customer_id=98468631, features=SparseVector(1666, {0: 1.0, 1: 1.0, 5: 1.0, 8: 2.0, 13: 1.0, 19: 1.0, 22: 1.0, 24: 2.0, 27: 1.0, 28: 1.0, 46: 1.0, 50: 4.0, 54: 1.0, 55: 3.0, 60: 11.0, 75: 1.0, 85: 1.0, 98: 1.0, 133: 2.0, 144: 3.0, 156: 1.0, 163: 1.0, 165: 2.0, 166: 1.0, 208: 1.0, 226: 1.0, 284: 1.0, 287: 2.0, 400: 2.0, 401: 2.0, 478: 1.0, 485: 1.0, 512: 1.0, 833: 22.0, 834: 64.0, 838: 12.0, 841: 30.5, 846: 10.0, 852: 128.0, 855: 1.0, 857: 22.0, 860: 240.0, 861: 2.0, 879: 24.0, 883: 24.0, 887: 73.28, 888: 600.0, 893: 32.0, 908: 16.0, 918: 32.0, 931: 20.0, 966: 12.0, 977: 13.3, 989: 3.5, 996: 6.5, 998: 18.0, 999: 144.0, 1041: 16.9, 1059: 4.25, 1117: 10.0, 1120: 44.0, 1233: 1.0, 1234: 20.0, 1311: 70.0, 1318: 9.0, 1345: 4.0}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_pivot1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_avg_agg(['customer_id', 'brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_pivot2 = aggregate_pivot_to_sparse_vector(ddf_trans, id_column='customer_id', \n",
    "                                              pivot_column='brand', \n",
    "                                              aggs=total_avg_agg(['customer_id', 'brand']))                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_pivot2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_pivot12 = ddf_pivot1.join(ddf_pivot2, on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_pivot12.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler( inputCols=[\"category_features\", \"brand_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = assembler.transform(ddf_pivot12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = merge_features(ddfs=[ddf_pivot1,ddf_pivot2], join_column='customer_id', merge_column='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot agg customer_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_agg_customer_category = pivot_aggregate(ddf_transactions, grpby_columns=['customer_id','category'],\n",
    "                                            aggs=count_agg(grpby_columns),\n",
    "                                            pivot_column='category', pivot_filter_support=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_agg_customer_category.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_agg_customer_category.rdd.setName('ddf_agg_customer_category') \\\n",
    "                         .persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## merge with offers history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_agg_customer_category = rename_columns(ddf_agg_customer_category, prefix = 'left', separator='.', columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assemble_columns = list(set(ddf_agg_customer_category.columns) -set(filter_columns('left.*',ddf_agg_customer_category)) \\\n",
    "                    -set(ddf_offers_hist.columns) )+ cat_cols+num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemble_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers__ohagg_cid_category = ddf_offers_hist.join(ddf_agg_customer_category,\n",
    "                                                     on=['customer_id'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers__ohagg_cid_category = vecAssembler.transform(ddf_offers__ohagg_cid_category) \\\n",
    "                                             .select(['customer_id','features', 'repeater']+cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_ddf_to_csv(ddf_offers__ohagg_cid_category, spark_data_path+'ddf_offers__ohagg_cid_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers__ohagg_cid_category.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers_subm__ohagg_cid_category = ddf_offers_subm.join(ddf_agg_customer_category,\n",
    "                                                      on=['customer_id'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers_subm__ohagg_cid_category = vecAssembler.transform(ddf_offers_subm__ohagg_cid_category) \\\n",
    "                                    .select(['customer_id', 'features', F.lit('f').alias('repeater')]+cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_ddf_to_csv(ddf_offers_subm__ohagg_cid_category, spark_data_path+'ddf_offers_subm__ohagg_cid_category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot_aggregate(ddf, grpby_columns, pivot_column, aggs, pivot_filter_values=None, pivot_filter_support=None):\n",
    "    if pivot_filter_support and not pivot_filter_values:        \n",
    "        frequent = ddf.freqItems([pivot_column], support=pivot_filter_support).first().asDict()[pivot_column+'_freqItems']\n",
    "        pivot_filter_values = map(str,frequent)\n",
    "    \n",
    "    ddf_gr = ddf.groupBy(*grpby_columns)\n",
    "    ddf_pivot = ddf_gr.pivot(pivot_column, pivot_filter_values)\n",
    "    ddf_agg = ddf_pivot.agg(*aggs)\n",
    "    return ddf_agg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rename_columns(df, prefix='', suffix='', separator='_', columns=None):\n",
    "    prefix = prefix + separator if prefix else prefix\n",
    "    suffix = separator + suffix if suffix else suffix\n",
    "    columns = df.columns if columns is None else columns\n",
    "    df1 = df.select('*')\n",
    "    for c in columns:\n",
    "        df1 = df1.withColumnRenamed(c, prefix + c + suffix)\n",
    "    return df1       \n",
    "\n",
    "\n",
    "def filter_columns(expr, df):\n",
    "    import re\n",
    "    return filter(lambda c: re.match(expr,c), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
