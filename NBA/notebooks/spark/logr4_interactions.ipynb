{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/.pyenv/versions/anaconda2-2.4.1/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import sql\n",
    "from pyspark import SparkConf\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import GroupedData\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation  import *\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "from datasu.auc import *\n",
    "from datasu.dicts import *\n",
    "from datasu.files import *\n",
    "from datasu.pandas import *\n",
    "from datasu.persist import *\n",
    "from datasu.spark import *\n",
    "from datasu.patsy import *\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.app.name', u'logr3'),\n",
       " (u'spark.master', u'spark://spark1.ea.lab:7077'),\n",
       " (u'spark.cores.max', u'28'),\n",
       " (u'spark.driver.memory', u'12g'),\n",
       " (u'spark.python.worker.memory', u'2g'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.executor.memory', u'4g'),\n",
       " (u'spark.executor.extraJavaOptions',\n",
       "  u'-XX:+PrintGCDetails -XX:+UseCompressedOops'),\n",
       " (u'spark.jars',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.driver.maxResultSize', u'5g'),\n",
       " (u'spark.files',\n",
       "  u'file:/home/ds/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/ds/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ds/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.executor.memory', '4g')\n",
    "conf.set('spark.driver.memory', '12g')\n",
    "conf.set('spark.python.worker.memory', '2g')\n",
    "conf.set(\"spark.driver.maxResultSize\", \"5g\")\n",
    "conf.set(\"spark.cores.max\", 28)\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set('spark.executor.extraJavaOptions', '-XX:+PrintGCDetails -XX:+UseCompressedOops')\n",
    "\n",
    "conf.setAppName('logr3')\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print 'spark context not exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqc = pyspark.SQLContext(sc)\n",
    "sqlContext = sqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = sqc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_data_path = '/home/ds/dev/data/Kagle-ValuesShoppers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_data_path = 'file://'+ base_data_path + 'spark_data/'\n",
    "transactions_name = 'transactions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = csv_reader.load(spark_data_path+'ddf_trans_agg_history', samplingRatio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = ddf_trans_agg_history.withColumnRenamed('repeater_calc', 'repeater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_coupons = pd.read_csv(base_data_path+'offers')[['offer','category','company','brand','offervalue','quantity']]\n",
    "df_offers_ids = pd.read_csv(base_data_path+'trainHistory').rename(columns={'id': 'customer_id'})\n",
    "df_offers_ids_submission = pd.read_csv(base_data_path+'testHistory').rename(columns={'id': 'customer_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_offers_all = pd.merge(df_offers_ids, df_coupons, on=['offer'])\n",
    "df_offers_all = df_offers_all[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity','repeattrips','repeater']]\n",
    "\n",
    "df_offers_all_submission = pd.merge(df_offers_ids_submission, df_coupons, on=['offer'])\n",
    "df_offers_all_submission = df_offers_all_submission[['customer_id','chain','offer','market','category','company','brand','offerdate','offervalue','quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_folder = '/home/ds/dev/data/Kagle-ValuesShoppers/'\n",
    "load_variables(path=main_folder+'working_data', variables=['df_offers__trans_aggs', 'df_offers_submission__trans_aggs']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers__trans_aggs = sqc.createDataFrame(df_offers__trans_aggs).na.fill(0)\n",
    "ddf_offers_submission__trans_aggs = sqc.createDataFrame(df_offers_submission__trans_aggs).na.fill(0)\n",
    "# ddf_offers__trans_aggs = ddf_offers__trans_aggs.drop('repeattrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['chain','market','category','company','brand']\n",
    "agg_cols = filter(lambda c: re.match(\"agg_*\",c), ddf_offers__trans_aggs.columns)\n",
    "num_cols = ['offervalue','quantity']\n",
    "\n",
    "ddf_offers__trans_aggs = convert_columns_to_type(ddf_offers__trans_aggs, categorical_cols, StringType)\n",
    "ddf_offers_submission__trans_aggs = convert_columns_to_type(ddf_offers_submission__trans_aggs, categorical_cols, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = convert_columns_to_type(ddf_trans_agg_history, categorical_cols, StringType)\n",
    "ddf_trans_agg_history = ddf_trans_agg_history.withColumn('repeater', F.substring(ddf_trans_agg_history.repeater, 0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plus_categorical_expr = partial(plus_expr, expr='%s')\n",
    "# categorical_cols_plus = plus_categorical_expr(categorical_cols)\n",
    "\n",
    "# brand_interactions = \"(%s):(%s)\" % (plus_agg_columns_by_infix('brand',agg_cols), categorical_cols_plus)\n",
    "# category_interactions = \"(%s):(%s)\" % (plus_agg_columns_by_infix('category',agg_cols), categorical_cols_plus)\n",
    "# num_interactions = \"(%s):(%s)\" % (plus_expr(agg_cols), plus_expr(num_cols))\n",
    "\n",
    "# R_expr = plus_expr([brand_interactions,category_interactions,num_interactions])\n",
    "# R_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build prepare pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf1 = RFormula(formula=\"repeater ~ . - repeattrips\", featuresCol=\"features\", labelCol=\"label\")\n",
    "# rf1 = RFormula(formula=\"repeater ~ agg_customer_id_brand_customer_id_count\", featuresCol=\"features\", labelCol=\"label\")\n",
    "rf1 = RFormula(formula=\"repeater ~ category:. + brand:. -repeattrips -chain -offer -market -company -offerdate -offervalue -quantity\", featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "\n",
    "\n",
    "pipe_transform1 = Pipeline(stages=[rf1])\n",
    "\n",
    "std_scale = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=False)\n",
    "\n",
    "px = PolynomialExpansion(degree=2, inputCol=\"features_scaled\", outputCol=\"features_poly\")\n",
    "\n",
    "# pipe_prepare1 = Pipeline(stages=[std_scale, px])\n",
    "pipe_prepare1 = Pipeline(stages=[std_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"label\", predictionCol=\"prediction\", regParam=0.1, elasticNetParam=0.3)\n",
    "\n",
    "# , \n",
    "#                          maxIter=100, regParam=0.1, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \n",
    "#                          thresholds=None, probabilityCol=\"probability\",  \n",
    "#                          rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_lr1 = Pipeline(stages=[pipe_prepare1, lr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_offers_submission__trans_aggs = ddf_offers_submission__trans_aggs.withColumn('repeater',F.lit('f'))\n",
    "ddf_offers_union__trans_aggs = ddf_offers__trans_aggs.unionAll(\n",
    "                            ddf_offers_submission__trans_aggs.select(*ddf_offers__trans_aggs.columns))\n",
    "\n",
    "ddf_offers_union__trans_aggs = ddf_offers_union__trans_aggs.drop('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_transform2 = pipe_transform1.fit(ddf_offers_union__trans_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data = model_transform2.transform(ddf_offers__trans_aggs)\n",
    "ddf_data = ddf_data.select(['features','label']) #, 'repeattrips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_train, ddf_data_test = ddf_data.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ddf_data_train.groupBy('label').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr2 = pipe_lr1.fit(ddf_data_train) #, params={'weightCol':\"repeattrips\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1 = model_lr2.transform(ddf_data_test, params={'threshold':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_test_res1 = ddf_data_test_res1.select(['rawPrediction', get_index_from_vector()('probability', F.lit(1)).alias('probability'), 'prediction', 'label']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_test_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_auc(df_data_test_res1.label, df_data_test_res1.probability.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder().build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=pipe_lr1, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "cvModel = cv.fit(ddf_data_train)\n",
    "evaluator.evaluate(cvModel.transform(ddf_data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit big data!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history = ddf_trans_agg_history.drop('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history_union_offers = ddf_trans_agg_history.unionAll(ddf_offers_union__trans_aggs.select(*ddf_trans_agg_history.columns))\n",
    "ddf_trans_agg_history_union_offers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_transform1 = pipe_transform1.fit(ddf_trans_agg_history_union_offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data = model_transform1.transform(ddf_trans_agg_history)\n",
    "ddf_data = ddf_data.select(['features','label']) #, 'repeattrips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_train = ddf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test = model_transform1.transform(ddf_offers__trans_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ddf_data_train, ddf_data_test = ddf_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_trans_agg_history_union_offers.groupBy('repeater').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr1 = pipe_lr1.fit(ddf_data_train) #, params={'weightCol':\"repeattrips\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1 = model_lr1.transform(ddf_data_test, params={'threshold':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_test_res1 = ddf_data_test_res1.select(['rawPrediction', get_index_from_vector()('probability', F.lit(1)).alias('probability'), 'prediction', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_data_test_res1 = ddf_data_test_res1.sample(False, fraction=0.1).toPandas()\n",
    "df_data_test_res1 = ddf_data_test_res1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_test_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_auc(df_data_test_res1.label, df_data_test_res1.probability.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_submission = model_transform2.transform(ddf_offers_submission__trans_aggs).select(['customer_id','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf_data_submission_res1 = model_lr2.transform(ddf_data_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1 = ddf_data_submission_res1.select(\n",
    "                [F.col('customer_id').alias('id'), get_index_from_vector()('probability', F.lit(1)).alias('repeatProbability')]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_submission_res1.to_csv(path_or_buf=main_folder+'submission/'+'submission_spark18_LR1_bd', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "output = ParamGridBuilder() \\\n",
    "         .baseOn({lr.labelCol: 'l'}) \\\n",
    "         .baseOn([lr.predictionCol, 'p']) \\\n",
    "         .addGrid(lr.regParam, [1.0, 2.0]) \\\n",
    "         .addGrid(lr.maxIter, [1, 5]) \\\n",
    "         .build()\n",
    "            \n",
    "expected = [\n",
    "         {lr.regParam: 1.0, lr.maxIter: 1, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 2.0, lr.maxIter: 1, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 1.0, lr.maxIter: 5, lr.labelCol: 'l', lr.predictionCol: 'p'},\n",
    "         {lr.regParam: 2.0, lr.maxIter: 5, lr.labelCol: 'l', lr.predictionCol: 'p'}]\n",
    "\n",
    "len(output) == len(expected)\n",
    "all([m in expected for m in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "dataset = sqlContext.createDataFrame(\n",
    "     [(Vectors.dense([0.0]), 0.0),\n",
    "      (Vectors.dense([0.4]), 1.0),\n",
    "      (Vectors.dense([0.5]), 0.0),\n",
    "      (Vectors.dense([0.6]), 1.0),\n",
    "      (Vectors.dense([1.0]), 1.0)] * 10,\n",
    "     [\"features\", \"label\"])\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(dataset)\n",
    "evaluator.evaluate(cvModel.transform(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play with RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqc.createDataFrame([\n",
    "     (1.0, 1.0, \"a\",\"q\"),\n",
    "     (0.0, 2.0, \"b\",\"w\"),\n",
    "     (0.0, 3.0, \"a\",\"q\"),    \n",
    " ], [\"y\", \"x\", \"cat1\",\"cat2\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RFormula(formula=\"y ~ cat1\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df1 = rf.fit(df).transform(df)\n",
    "df1.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1+cat2\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df2 = rf.fit(df).transform(df)\n",
    "df2.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1:cat2\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df3 = rf.fit(df).transform(df)\n",
    "df3.show()\n",
    "\n",
    "rf = RFormula(formula=\"y ~ cat1:x + cat2:x\", featuresCol=\"features\", labelCol=\"label\")\n",
    "df4 = rf.fit(df).transform(df)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(df1)\n",
    "scalerModel.transform(df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "df = sc.parallelize([\n",
    "     Row(label=1.0, weight=2.0, features=Vectors.dense(1.0)),\n",
    "     Row(label=0.0, weight=2.0, features=Vectors.sparse(1, [], []))]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense(-1.0))]).toDF()\n",
    "result = model.transform(test0).head()\n",
    "result.prediction\n",
    "\n",
    "result.probability\n",
    "\n",
    "result.rawPrediction\n",
    "\n",
    "test1 = sc.parallelize([Row(features=Vectors.sparse(1, [0], [1.0]))]).toDF()\n",
    "model.transform(test1).head().prediction\n",
    "\n",
    "lr.setParams(\"vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
